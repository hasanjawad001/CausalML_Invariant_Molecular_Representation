{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa14dfe7-ce35-42c4-9774-d5c738b6b7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import ray\n",
    "from itertools import combinations\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import math\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge, ElasticNet\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.decomposition import PCA\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import sys\n",
    "import os\n",
    "\n",
    "original_stdout = sys.stdout\n",
    "sys.stdout = open('logger.log', 'a')\n",
    "# sys.stdout = original_stdout\n",
    "\n",
    "trial_no = 0\n",
    "seed = 42 + trial_no\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "curdir = 'curdir/'\n",
    "\n",
    "####################################\n",
    "## cathub\n",
    "####################################\n",
    "\n",
    "df_cat1 = pd.read_pickle(curdir + 'datasets/reactions_info_df.pickle')\n",
    "\n",
    "list_scols = ['s' + str(i) for i in range(9)]\n",
    "list_pcols = ['p' + str(i) for i in range(768)]\n",
    "list_cols = ['sc', 'facet'] + list_scols + list_pcols + ['nre']\n",
    "df_cat1 = df_cat1[list_cols]\n",
    "\n",
    "print(df_cat1.shape)\n",
    "\n",
    "# df_cat1.head(2)\n",
    "\n",
    "unique_groups_df_cat1 = df_cat1[['sc', 'facet']].drop_duplicates()\n",
    "num_unique_groups_df_cat1 = unique_groups_df_cat1.shape[0]\n",
    "print(f\"Number of distinct groups for ['sc', 'facet'] in df_cat1: {num_unique_groups_df_cat1}\")\n",
    "\n",
    "####################################\n",
    "## ocp\n",
    "####################################\n",
    "\n",
    "df_ocp1 = pd.read_parquet(curdir + 'datasets/ocp_reactions_info_df.parquet')\n",
    "\n",
    "# df_ocp1.head(2)\n",
    "\n",
    "def convert_to_int(x):\n",
    "    try:\n",
    "        return str(''.join(map(str, x)))\n",
    "    except ValueError:\n",
    "        print(f\"Error with value: {x}\")\n",
    "        return x\n",
    "    \n",
    "df_ocp1.rename(columns={'energy': 'nre'}, inplace=True)\n",
    "df_ocp1['miller_index'] = df_ocp1['miller_index'].apply(convert_to_int)\n",
    "list_scols = ['s' + str(i) for i in range(9)]\n",
    "list_pcols = ['p' + str(i) for i in range(768)]\n",
    "list_cols = ['bulk_mpid', 'miller_index'] + list_scols + list_pcols + ['nre']\n",
    "df_ocp1 = df_ocp1[list_cols]\n",
    "\n",
    "print(df_ocp1.shape)\n",
    "\n",
    "# df_ocp1.head(2)\n",
    "\n",
    "unique_groups_df_ocp1 = df_ocp1[['bulk_mpid', 'miller_index']].drop_duplicates()\n",
    "num_unique_groups_df_ocp1 = unique_groups_df_ocp1.shape[0]\n",
    "print(f\"Number of distinct groups for ['bulk_mpid', 'miller_index'] in df_ocp1: {num_unique_groups_df_ocp1}\")\n",
    "\n",
    "####################################\n",
    "## env creation\n",
    "####################################\n",
    "\n",
    "## do: create env for cathub\n",
    "\n",
    "df = df_cat1\n",
    "\n",
    "# Create a new 'group' column in df that combines 'sc' and 'facet' \n",
    "df['group'] = list(zip(df['sc'], df['facet']))\n",
    "\n",
    "# Find distinct groups and calculate the number of train/test groups\n",
    "distinct_values_df = df[['group']].drop_duplicates().reset_index(drop=True)\n",
    "list_env_group = [tuple(x) for x in distinct_values_df['group'].values]\n",
    "n_env_total = len(list_env_group)\n",
    "n_env_train = int(0.75 * n_env_total) # Update when necessary\n",
    "train_env_groups = set(random.sample(list_env_group, n_env_train))\n",
    "test_env_groups = set(list_env_group) - train_env_groups\n",
    "\n",
    "# Create list of dataframes for each training group\n",
    "list_env_train_cat1 = [df[df['group'] == grp].reset_index(drop=True) for grp in train_env_groups]\n",
    "\n",
    "# Concatenate the list to form a single dataframe for the training set\n",
    "df_train_cat1 = pd.concat(list_env_train_cat1, ignore_index=True)\n",
    "\n",
    "# Get all the rows that don't belong to the training set groups\n",
    "mask_train = df['group'].isin(train_env_groups)\n",
    "df_test_cat1 = df[~mask_train].reset_index(drop=True)\n",
    "\n",
    "# Cleanup: Drop the 'group' column as it's no longer needed\n",
    "df.drop(columns=['group'], inplace=True)\n",
    "df_train_cat1.drop(columns=['group'], inplace=True)\n",
    "df_test_cat1.drop(columns=['group'], inplace=True)\n",
    "\n",
    "print(len(list_env_train_cat1), len(train_env_groups), len(test_env_groups), n_env_total)\n",
    "print(df_train_cat1.shape, df_test_cat1.shape, df.shape)\n",
    "\n",
    "# Check the distinct groups\n",
    "distinct_groups_train = df_train_cat1[['sc', 'facet']].drop_duplicates().reset_index(drop=True)\n",
    "distinct_groups_test = df_test_cat1[['sc', 'facet']].drop_duplicates().reset_index(drop=True)\n",
    "print(len(distinct_groups_train), len(distinct_groups_test))\n",
    "\n",
    "## do: create env for ocp\n",
    "\n",
    "df = df_ocp1\n",
    "\n",
    "# Create a new 'group' column in df that combines 'bulk_mpid' and 'miller_index' \n",
    "df['group'] = list(zip(df['bulk_mpid'], df['miller_index']))\n",
    "\n",
    "# Find distinct groups and calculate the number of train/test groups\n",
    "distinct_values_df = df[['group']].drop_duplicates().reset_index(drop=True)\n",
    "list_env_group = [tuple(x) for x in distinct_values_df['group'].values]\n",
    "n_env_total = len(list_env_group)\n",
    "# n_env_train = int(0.75 * n_env_total) # Update when necessary\n",
    "train_env_groups = set(random.sample(list_env_group, n_env_train))\n",
    "test_env_groups = set(list_env_group) - train_env_groups\n",
    "\n",
    "# Create list of dataframes for each training group\n",
    "list_env_train_ocp1 = [df[df['group'] == grp].reset_index(drop=True) for grp in train_env_groups]\n",
    "\n",
    "# Concatenate the list to form a single dataframe for the training set\n",
    "df_train_ocp1 = pd.concat(list_env_train_ocp1, ignore_index=True)\n",
    "\n",
    "# Get all the rows that don't belong to the training set groups\n",
    "mask_train = df['group'].isin(train_env_groups)\n",
    "df_test_ocp1 = df[~mask_train].reset_index(drop=True)\n",
    "\n",
    "# Cleanup: Drop the 'group' column as it's no longer needed\n",
    "df.drop(columns=['group'], inplace=True)\n",
    "df_train_ocp1.drop(columns=['group'], inplace=True)\n",
    "df_test_ocp1.drop(columns=['group'], inplace=True)\n",
    "\n",
    "print(len(list_env_train_ocp1), len(train_env_groups), len(test_env_groups), n_env_total)\n",
    "print(df_train_ocp1.shape, df_test_ocp1.shape, df.shape)\n",
    "\n",
    "# Check the distinct groups\n",
    "distinct_groups_train = df_train_ocp1[['bulk_mpid', 'miller_index']].drop_duplicates().reset_index(drop=True)\n",
    "distinct_groups_test = df_test_ocp1[['bulk_mpid', 'miller_index']].drop_duplicates().reset_index(drop=True)\n",
    "print(len(distinct_groups_train), len(distinct_groups_test))\n",
    "\n",
    "#############################################################################\n",
    "## experiment with (cathub, ocp) X (ridge, elastic, krr, svr) X (org, pca, siamese) \n",
    "#############################################################################\n",
    "\n",
    "# Models and hyperparameters\n",
    "models = {\n",
    "    'Ridge Regression': {\n",
    "        'model': Ridge(),\n",
    "        'params': {\n",
    "            'alpha': [0.1, 1, 10],\n",
    "        }\n",
    "    },\n",
    "    'Elastic Regression': {\n",
    "        'model': ElasticNet(),\n",
    "        'params': {\n",
    "            'alpha': [0.1, 1, 10],\n",
    "            'l1_ratio': [0.3, 0.5, 0.7]\n",
    "        }\n",
    "    },\n",
    "    'Kernel Ridge Regression': {\n",
    "        'model': KernelRidge(),\n",
    "        'params': {\n",
    "            'alpha': [0.1, 1, 10],\n",
    "            'kernel': ['linear', 'polynomial', 'rbf'],\n",
    "            'degree': [2, 3, 4]\n",
    "        }\n",
    "    },\n",
    "    'Support Vector Regression': {\n",
    "        'model': SVR(),\n",
    "        'params': {\n",
    "            'C': [0.1, 1, 10],\n",
    "            'kernel': ['linear', 'rbf'],\n",
    "            'gamma': ['auto', 'scale']\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "class SiameseNetwork(torch.nn.Module):\n",
    "    def __init__(self, len_embedding, abstract_len_embedding, n_hidden_node=32):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.loss = nn.L1Loss(reduction=\"mean\") \n",
    "        self.len_embedding = len_embedding\n",
    "        self.abstract_len_embedding = abstract_len_embedding  \n",
    "        self.n_hidden_node = n_hidden_node\n",
    "        self.nn_reg = nn.Sequential(\n",
    "            nn.Linear(self.len_embedding, self.abstract_len_embedding),\n",
    "        )\n",
    "        self.nn_final_reg = nn.Sequential(\n",
    "            nn.Linear(self.abstract_len_embedding * 2, self.n_hidden_node),nn.ReLU(inplace=True),nn.BatchNorm1d(self.n_hidden_node),nn.Dropout(p=0.1),\n",
    "            nn.Linear(self.n_hidden_node, int(self.n_hidden_node/4)),nn.ReLU(inplace=True),nn.BatchNorm1d(int(self.n_hidden_node/4)),nn.Dropout(p=0.1),\n",
    "            nn.Linear(int(self.n_hidden_node/4), 1),\n",
    "        )\n",
    "\n",
    "    def forward_reg(self, x):\n",
    "        output = self.nn_reg(x)\n",
    "        return output\n",
    "\n",
    "    def forward_final_reg(self, x):\n",
    "        output = self.nn_final_reg(x)\n",
    "        return output\n",
    "\n",
    "    def forward(self, fp1, fp2):\n",
    "        a = self.forward_reg(fp1)\n",
    "        b = self.forward_reg(fp2)\n",
    "        x = torch.cat([a, b], dim=1)  # hstack\n",
    "        output = self.forward_final_reg(x)\n",
    "        return output\n",
    "\n",
    "def create_comparison_data(df):\n",
    "    n_samples = df.shape[0]\n",
    "    feature_data = []\n",
    "    target_data = []\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        for j in range(i+1, n_samples):\n",
    "            sample_i = df.iloc[i, 2:779].values\n",
    "            sample_j = df.iloc[j, 2:779].values\n",
    "            difference_i_j = df.iloc[i, 779] - df.iloc[j, 779]\n",
    "            difference_j_i = df.iloc[j, 779] - df.iloc[i, 779]\n",
    "            \n",
    "            combined_features_i_j = np.concatenate([sample_i, sample_j])\n",
    "            combined_features_j_i = np.concatenate([sample_j, sample_i])\n",
    "            \n",
    "            feature_data.append(combined_features_i_j)\n",
    "            target_data.append(difference_i_j)\n",
    "            \n",
    "            feature_data.append(combined_features_j_i)\n",
    "            target_data.append(difference_j_i)\n",
    "    \n",
    "    return feature_data, target_data\n",
    "\n",
    "## do: (cathub) x (ridge, elastic, krr, svr) x (org, pca, siamese)\n",
    "\n",
    "# Separate features and targets\n",
    "X_train = df_train_cat1.iloc[:, 2:-1]\n",
    "y_train = df_train_cat1.iloc[:, -1]\n",
    "X_test = df_test_cat1.iloc[:, 2:-1]\n",
    "y_test = df_test_cat1.iloc[:, -1]\n",
    "\n",
    "# Standardize the features\n",
    "scaler_X = StandardScaler().fit(X_train)\n",
    "X_train_std = scaler_X.transform(X_train)\n",
    "X_test_std = scaler_X.transform(X_test)\n",
    "\n",
    "## original\n",
    "\n",
    "X_train_org = X_train_std\n",
    "X_test_org = X_test_std\n",
    "\n",
    "## pca - Apply PCA and retain 90% variance\n",
    "pca = PCA(n_components=0.9)\n",
    "X_train_pca = pca.fit_transform(X_train_std)\n",
    "X_test_pca = pca.transform(X_test_std)\n",
    "print('X_train_pca shape: ', X_train_pca.shape)\n",
    "\n",
    "## siamese\n",
    "\n",
    "len_embedding, abstract_len_embedding = int(X_train_org.shape[1]), int(X_train_pca.shape[1])\n",
    "print('len_embedding, abstract_len_embedding: ', len_embedding, abstract_len_embedding)\n",
    "\n",
    "# Combine both lists of DataFrames\n",
    "all_dfs = list_env_train_cat1 + list_env_train_ocp1\n",
    "\n",
    "# Initialize empty lists for all feature data and target data\n",
    "all_feature_data = []\n",
    "all_target_data = []\n",
    "\n",
    "# Iterate over the combined list of DataFrames and create comparison data\n",
    "for dataframe in all_dfs:\n",
    "    feature_data, target_data = create_comparison_data(dataframe)\n",
    "    all_feature_data.extend(feature_data)\n",
    "    all_target_data.extend(target_data)\n",
    "\n",
    "# Convert lists to arrays\n",
    "X_senv = np.array(all_feature_data)\n",
    "y_senv = np.array(all_target_data)\n",
    "\n",
    "print('X_senv shape, y_senv shape: ', X_senv.shape, y_senv.shape)\n",
    "\n",
    "scaler = StandardScaler().fit(X_senv)\n",
    "X_senv_std = scaler.transform(X_senv)\n",
    "\n",
    "fp1 = torch.tensor(X_senv_std[:, :len_embedding], dtype=torch.float32)\n",
    "fp2 = torch.tensor(X_senv_std[:, len_embedding:], dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y_senv, dtype=torch.float32).unsqueeze(1)  # Convert y to tensor and add a dimension\n",
    "\n",
    "dataset = TensorDataset(fp1, fp2, y_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "n_hidden_nodes = [8]\n",
    "learning_rates = [0.01]\n",
    "num_epochs = 100  # Adjust as necessary\n",
    "\n",
    "best_model = None\n",
    "lowest_error = float('inf')\n",
    "\n",
    "for n_hidden_node in n_hidden_nodes:\n",
    "    for lr in learning_rates:\n",
    "        \n",
    "        model = SiameseNetwork(len_embedding, abstract_len_embedding, n_hidden_node)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            if epoch % int(num_epochs/10) == 0:\n",
    "                print(epoch)\n",
    "            for batch_fp1, batch_fp2, batch_y in dataloader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(batch_fp1, batch_fp2)\n",
    "                loss = model.loss(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "        # Calculate training error for the current hyperparameter combination\n",
    "        with torch.no_grad():\n",
    "            all_outputs = model(fp1, fp2)\n",
    "            train_error = model.loss(all_outputs, y_tensor).item()\n",
    "        \n",
    "        # Update best model and lowest error if needed\n",
    "        if train_error < lowest_error:\n",
    "            lowest_error = train_error\n",
    "            best_model = model\n",
    "\n",
    "print(f\"Best Model has a training error of: {lowest_error}\")\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train_std, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_std, dtype=torch.float32)\n",
    "\n",
    "# Use the model to transform the data\n",
    "X_train_siamese_tensor = best_model.nn_reg(X_train_tensor)\n",
    "X_test_siamese_tensor = best_model.nn_reg(X_test_tensor)\n",
    "\n",
    "# Convert the output tensors to NumPy arrays\n",
    "X_train_siamese = X_train_siamese_tensor.detach().numpy()\n",
    "X_test_siamese = X_test_siamese_tensor.detach().numpy()\n",
    "\n",
    "print('X_train_siamese shape, X_test_siamese shape', X_train_siamese.shape, X_test_siamese.shape)\n",
    "\n",
    "# Display mean absolute value of the target\n",
    "mean_absolute_target = y_test.abs().mean()\n",
    "print(f\"Mean Absolute Value of Target: {mean_absolute_target}\")\n",
    "\n",
    "# Tune hyperparameters and train models\n",
    "for iemb, (X_train, X_test) in enumerate([(X_train_org, X_test_org), (X_train_pca, X_test_pca), (X_train_siamese, X_test_siamese)]):\n",
    "    print()\n",
    "    print('=================================================')\n",
    "    print('iemb: ', iemb)\n",
    "    print('=================================================')\n",
    "    print()\n",
    "    for name, model_info in models.items():\n",
    "        grid_search = GridSearchCV(model_info['model'], model_info['params'], scoring='neg_mean_absolute_error', cv=5)\n",
    "        grid_search.fit(X_train, y_train)  # Changed to X_train instead of X_train_pca\n",
    "\n",
    "        # Use the best model for predictions\n",
    "        best_model = grid_search.best_estimator_\n",
    "        y_pred = best_model.predict(X_test)  # Changed to X_test instead of X_test_pca\n",
    "\n",
    "        # Mean Absolute Error\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        print(f\"{name} Mean Absolute Error with Best Hyperparameters: {mae}\")\n",
    "\n",
    "        # Plot true vs. predicted values\n",
    "        plt.figure(figsize=(4, 3))\n",
    "        plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "        plt.xlabel(\"True Values\")\n",
    "        plt.ylabel(\"Predictions\")\n",
    "        plt.title(f\"{name} - True vs. Predicted with Best Hyperparameters\")\n",
    "        plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\n",
    "        \n",
    "        # plt.show()\n",
    "        # Save the plot to the outputs/ directory\n",
    "        filename = curdir + 'outputs/' + str('cathub') + '_' + str(iemb) + '_' + str(name) + '.png'\n",
    "        plt.savefig(filename)\n",
    "        plt.close()  # Close the plot to free memory\n",
    "\n",
    "## do: (ocp) x (ridge, elastic, krr, svr) x (org, pca, siamese)\n",
    "\n",
    "# Separate features and targets\n",
    "X_train = df_train_ocp1.iloc[:, 2:-1]\n",
    "y_train = df_train_ocp1.iloc[:, -1]\n",
    "X_test = df_test_ocp1.iloc[:, 2:-1]\n",
    "y_test = df_test_ocp1.iloc[:, -1]\n",
    "\n",
    "# Standardize the features\n",
    "scaler_X = StandardScaler().fit(X_train)\n",
    "X_train_std = scaler_X.transform(X_train)\n",
    "X_test_std = scaler_X.transform(X_test)\n",
    "\n",
    "## original\n",
    "\n",
    "X_train_org = X_train_std\n",
    "X_test_org = X_test_std\n",
    "\n",
    "## pca - Apply PCA and retain 90% variance\n",
    "pca = PCA(n_components=0.9)\n",
    "X_train_pca = pca.fit_transform(X_train_std)\n",
    "X_test_pca = pca.transform(X_test_std)\n",
    "print('X_train_pca shape: ', X_train_pca.shape)\n",
    "\n",
    "## siamese\n",
    "\n",
    "len_embedding, abstract_len_embedding = int(X_train_org.shape[1]), int(X_train_pca.shape[1])\n",
    "print('len_embedding, abstract_len_embedding: ', len_embedding, abstract_len_embedding)\n",
    "\n",
    "# Combine both lists of DataFrames\n",
    "all_dfs = list_env_train_cat1 + list_env_train_ocp1\n",
    "\n",
    "# Initialize empty lists for all feature data and target data\n",
    "all_feature_data = []\n",
    "all_target_data = []\n",
    "\n",
    "# Iterate over the combined list of DataFrames and create comparison data\n",
    "for dataframe in all_dfs:\n",
    "    feature_data, target_data = create_comparison_data(dataframe)\n",
    "    all_feature_data.extend(feature_data)\n",
    "    all_target_data.extend(target_data)\n",
    "\n",
    "# Convert lists to arrays\n",
    "X_senv = np.array(all_feature_data)\n",
    "y_senv = np.array(all_target_data)\n",
    "\n",
    "print('X_senv shape, y_senv shape: ', X_senv.shape, y_senv.shape)\n",
    "\n",
    "scaler = StandardScaler().fit(X_senv)\n",
    "X_senv_std = scaler.transform(X_senv)\n",
    "\n",
    "fp1 = torch.tensor(X_senv_std[:, :len_embedding], dtype=torch.float32)\n",
    "fp2 = torch.tensor(X_senv_std[:, len_embedding:], dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y_senv, dtype=torch.float32).unsqueeze(1)  # Convert y to tensor and add a dimension\n",
    "\n",
    "dataset = TensorDataset(fp1, fp2, y_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "n_hidden_nodes = [8]\n",
    "learning_rates = [0.01]\n",
    "num_epochs = 100  # Adjust as necessary\n",
    "\n",
    "best_model = None\n",
    "lowest_error = float('inf')\n",
    "\n",
    "for n_hidden_node in n_hidden_nodes:\n",
    "    for lr in learning_rates:\n",
    "        \n",
    "        model = SiameseNetwork(len_embedding, abstract_len_embedding, n_hidden_node)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            if epoch % int(num_epochs/10) == 0:\n",
    "                print(epoch)\n",
    "            for batch_fp1, batch_fp2, batch_y in dataloader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(batch_fp1, batch_fp2)\n",
    "                loss = model.loss(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "        # Calculate training error for the current hyperparameter combination\n",
    "        with torch.no_grad():\n",
    "            all_outputs = model(fp1, fp2)\n",
    "            train_error = model.loss(all_outputs, y_tensor).item()\n",
    "        \n",
    "        # Update best model and lowest error if needed\n",
    "        if train_error < lowest_error:\n",
    "            lowest_error = train_error\n",
    "            best_model = model\n",
    "\n",
    "print(f\"Best Model has a training error of: {lowest_error}\")\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train_std, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_std, dtype=torch.float32)\n",
    "\n",
    "# Use the model to transform the data\n",
    "X_train_siamese_tensor = best_model.nn_reg(X_train_tensor)\n",
    "X_test_siamese_tensor = best_model.nn_reg(X_test_tensor)\n",
    "\n",
    "# Convert the output tensors to NumPy arrays\n",
    "X_train_siamese = X_train_siamese_tensor.detach().numpy()\n",
    "X_test_siamese = X_test_siamese_tensor.detach().numpy()\n",
    "\n",
    "print('X_train_siamese shape, X_test_siamese shape', X_train_siamese.shape, X_test_siamese.shape)\n",
    "\n",
    "# Display mean absolute value of the target\n",
    "mean_absolute_target = y_test.abs().mean()\n",
    "print(f\"Mean Absolute Value of Target: {mean_absolute_target}\")\n",
    "\n",
    "# Tune hyperparameters and train models\n",
    "for iemb, (X_train, X_test) in enumerate([(X_train_org, X_test_org), (X_train_pca, X_test_pca), (X_train_siamese, X_test_siamese)]):\n",
    "    print()\n",
    "    print('=================================================')\n",
    "    print('iemb: ', iemb)\n",
    "    print('=================================================')\n",
    "    print()\n",
    "    for name, model_info in models.items():\n",
    "        grid_search = GridSearchCV(model_info['model'], model_info['params'], scoring='neg_mean_absolute_error', cv=5)\n",
    "        grid_search.fit(X_train, y_train)  # Changed to X_train instead of X_train_pca\n",
    "\n",
    "        # Use the best model for predictions\n",
    "        best_model = grid_search.best_estimator_\n",
    "        y_pred = best_model.predict(X_test)  # Changed to X_test instead of X_test_pca\n",
    "\n",
    "        # Mean Absolute Error\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        print(f\"{name} Mean Absolute Error with Best Hyperparameters: {mae}\")\n",
    "\n",
    "        # Plot true vs. predicted values\n",
    "        plt.figure(figsize=(4, 3))\n",
    "        plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "        plt.xlabel(\"True Values\")\n",
    "        plt.ylabel(\"Predictions\")\n",
    "        plt.title(f\"{name} - True vs. Predicted with Best Hyperparameters\")\n",
    "        plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\n",
    "        \n",
    "        # plt.show()\n",
    "        # Save the plot to the outputs/ directory\n",
    "        filename = curdir + 'outputs/' + str('ocp') + '_' + str(iemb) + '_' + str(name) + '.png'\n",
    "        plt.savefig(filename)\n",
    "        plt.close()  # Close the plot to free memory\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260112b5-87d8-4f46-9983-b0bacf749cff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

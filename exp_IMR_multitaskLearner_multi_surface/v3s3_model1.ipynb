{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9bfca20-29db-42d5-b0d5-fbab70046eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. extract all slabs from ocp and cathub(mamun)\n",
    "## 2. generate slab descriptor with dimenet++\n",
    "## 3. extract all product from ocp and cathub(mamun)\n",
    "## 4. generate product descriptor with chEMBL\n",
    "## 5. run experiments-\n",
    "##    - multitask learner -> (cathub, ocp) x (xgboost) x (original 1024+1024, pca ncomponents, imr ncomponents)\n",
    "##    - solves n^2 to n, solves descriptor generation for slab/surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8e9b2f4-f402-4144-9cf9-f2420f8b7850",
   "metadata": {},
   "outputs": [],
   "source": [
    "## install\n",
    "# !pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ac7d380-e4a0-4341-8aaa-7e19096d13b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "from sklearn.decomposition import PCA\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import DataParallel\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from itertools import cycle\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a8c9325-4d7b-4c84-be01-ff950023aa2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## variables\n",
    "seed = 42\n",
    "ratio_test = 0.25\n",
    "exp_name = 'v3s3_model1'\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "ntrial = 1 ## change this to 10\n",
    "ncomp_perc = 0.9949"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9aeb9af7-d211-4d43-beda-ebbad6cffa62",
   "metadata": {},
   "outputs": [],
   "source": [
    "## class\n",
    "class InvariantModel(nn.Module):\n",
    "    def __init__(self, len_embedding, abstract_len_embedding):\n",
    "        super(InvariantModel, self).__init__()\n",
    "        self.loss = nn.L1Loss(reduction=\"mean\") \n",
    "        self.len_embedding = len_embedding\n",
    "        self.abstract_len_embedding = abstract_len_embedding  \n",
    "        self.encoder = nn.Sequential(\n",
    "            ## 1536 to ~1125\n",
    "            nn.Linear(self.len_embedding, int(self.abstract_len_embedding*3)),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(int(self.abstract_len_embedding*3)), \n",
    "            ## ~1125 to ~750\n",
    "            nn.Linear(int(self.abstract_len_embedding*3), int(self.abstract_len_embedding*2)),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(int(self.abstract_len_embedding*2)),\n",
    "            ## ~750 to ~375\n",
    "            nn.Linear(int(self.abstract_len_embedding*2), self.abstract_len_embedding),\n",
    "        )        \n",
    "        self.head1 = nn.Linear(self.abstract_len_embedding, 1)  # For dataset 1\n",
    "        self.head2 = nn.Linear(self.abstract_len_embedding, 1)  # For dataset 2\n",
    "\n",
    "    def forward(self, x, dataset_id):\n",
    "        x = self.encoder(x)\n",
    "        if dataset_id == 1:\n",
    "            return self.head1(x)\n",
    "        else:\n",
    "            return self.head2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa376bc5-cdfc-442c-800b-fe712773157f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## function\n",
    "\n",
    "def get_model_invariant(train_loader1, train_loader2, len_embedding, abstract_len_embedding, trial_no):    \n",
    "    print(f'len_embedding: {len_embedding}, abstract_len_embedding: {abstract_len_embedding}')\n",
    "    \n",
    "    _lr, num_iterations = 1e-4, 10 ## change this to 10000\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model_invariant = InvariantModel(len_embedding, abstract_len_embedding)\n",
    "    if torch.cuda.is_available():\n",
    "        model_invariant = DataParallel(model_invariant)\n",
    "    model_invariant.to(device)\n",
    "    optimizer_invariant = torch.optim.Adam(model_invariant.parameters(), lr=_lr)\n",
    "\n",
    "    model_invariant.train()\n",
    "    for epoch in range(num_iterations):\n",
    "        total_loss = 0.0\n",
    "\n",
    "        # Cycle through the smaller dataset\n",
    "        train_loader1_cycle = cycle(train_loader1)\n",
    "        train_loader2_cycle = cycle(train_loader2)\n",
    "        # Determine the maximum number of batches between the two datasets\n",
    "        max_batches = max(len(train_loader1), len(train_loader2))\n",
    "\n",
    "        for i in range(max_batches):\n",
    "            # Get batch from each dataset; cycle the smaller dataset\n",
    "            data1 = next(train_loader1_cycle)\n",
    "            data2 = next(train_loader2_cycle)\n",
    "            # Process dataset 1\n",
    "            inputs1, labels1 = data1[0].to(device), data1[1].to(device)\n",
    "            outputs1 = model_invariant(inputs1, dataset_id=1).squeeze()\n",
    "            loss1 = model_invariant.module.loss(outputs1, labels1) if isinstance(model_invariant, DataParallel) else model_invariant.loss(outputs1, labels1)            \n",
    "            # Process dataset 2\n",
    "            inputs2, labels2 = data2[0].to(device), data2[1].to(device)\n",
    "            outputs2 = model_invariant(inputs2, dataset_id=2).squeeze()\n",
    "            loss2 = model_invariant.module.loss(outputs2, labels2) if isinstance(model_invariant, DataParallel) else model_invariant.loss(outputs2, labels2)            \n",
    "\n",
    "            optimizer_invariant.zero_grad()            \n",
    "            (loss1 + loss2).backward()\n",
    "            optimizer_invariant.step()\n",
    "            \n",
    "            total_loss += (loss1.item()+loss2.item())\n",
    "\n",
    "\n",
    "        if epoch % 1 == 0:\n",
    "            with open('v3/logger_' + exp_name + '_' + str(trial_no) + '.log', 'a+') as file1:\n",
    "                file1.writelines(f'epoch: {epoch}, total_loss: {total_loss:.6f}\\n\\n')\n",
    "                \n",
    "    return model_invariant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "980c7151-022f-435d-b453-7b72e4b90f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_pickle(f'v3/cathub_df.pickle')\n",
    "X1 = df1.iloc[:, :-1].values\n",
    "y1 = df1['energy'].values\n",
    "df2 = pd.read_pickle(f'v3/ocp_df.pickle')\n",
    "X2 = df2.iloc[:, :-1].values\n",
    "y2 = df2['energy'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24faf215-b964-42ba-b7cc-19ac00e9af24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "trial no: 0\n",
      "-- original --\n",
      "R2-score: 0.89\n",
      "R2-score: 0.63\n",
      "-- pca --\n",
      "ncomp1: 340\n",
      "R2-score: 0.90\n",
      "ncomp2: 401\n",
      "R2-score: 0.63\n",
      "-- invariant --\n",
      "len_embedding: 1536, abstract_len_embedding: 370\n",
      "R2-score: 0.95\n",
      "R2-score: 0.69\n"
     ]
    }
   ],
   "source": [
    "for trial_no in range(ntrial):\n",
    "    print()\n",
    "    print(f'trial no: {trial_no}')\n",
    "    \n",
    "    #####################################################################################################\n",
    "    ## original\n",
    "    #####################################################################################################   \n",
    "    print('-- original --')\n",
    "    ## cathub     \n",
    "    ## data\n",
    "    X_train1, X_test1, y_train1, y_test1 = train_test_split(X1, y1, test_size=ratio_test, random_state=seed+(trial_no*10))\n",
    "    # print(X_train1.shape, X_test1.shape, y_train1.shape, y_test1.shape)\n",
    "    ## model train\n",
    "    lr, depth, n_est = 0.2, 8, 500\n",
    "    # model1 = xgb.XGBRegressor(learning_rate=lr, max_depth=depth, n_estimators=n_est)\n",
    "    model1 = LinearRegression()\n",
    "    model1.fit(X_train1, y_train1)\n",
    "    ## model test\n",
    "    y_pred1 = model1.predict(X_test1)\n",
    "    mae1 = mean_absolute_error(y_test1, y_pred1)\n",
    "    r2score1 = r2_score(y_test1, y_pred1)\n",
    "    ## evaluation\n",
    "    # print(f'Mean Abs. Error: {mae1:.2f}')\n",
    "    print(f'R2-score: {r2score1:.2f}')\n",
    "    ## ocp \n",
    "    ## data\n",
    "    X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y2, test_size=ratio_test, random_state=seed+(trial_no*10))\n",
    "    # print(X_train2.shape, X_test2.shape, y_train2.shape, y_test2.shape)    \n",
    "    ## model train\n",
    "    lr, depth, n_est = 0.2, 8, 500\n",
    "    # model2 = xgb.XGBRegressor(learning_rate=lr, max_depth=depth, n_estimators=n_est)\n",
    "    model2 = LinearRegression()\n",
    "    model2.fit(X_train2, y_train2)\n",
    "    ## model test\n",
    "    y_pred2 = model2.predict(X_test2)\n",
    "    mae2 = mean_absolute_error(y_test2, y_pred2)\n",
    "    r2score2 = r2_score(y_test2, y_pred2)\n",
    "    ## evaluation\n",
    "    # print(f'Mean Abs. Error: {mae2:.2f}')\n",
    "    print(f'R2-score: {r2score2:.2f}')\n",
    "    \n",
    "    #####################################################################################################\n",
    "    ## pca\n",
    "    #####################################################################################################    \n",
    "    print('-- pca --')    \n",
    "    ## cathub     \n",
    "    ## data\n",
    "    pca1 = PCA(n_components=ncomp_perc)  # Keep 95% of variance\n",
    "    X_train1_pca = pca1.fit_transform(X_train1)\n",
    "    X_test1_pca = pca1.transform(X_test1)    \n",
    "    ncomp1 = pca1.n_components_\n",
    "    ## model train\n",
    "    lr, depth, n_est = 0.2, 8, 500\n",
    "    # model1 = xgb.XGBRegressor(learning_rate=lr, max_depth=depth, n_estimators=n_est)\n",
    "    model1 = LinearRegression()\n",
    "    model1.fit(X_train1_pca, y_train1)\n",
    "    ## model test\n",
    "    y_pred1 = model1.predict(X_test1_pca)\n",
    "    mae1 = mean_absolute_error(y_test1, y_pred1)\n",
    "    r2score1 = r2_score(y_test1, y_pred1)\n",
    "    ## evaluation\n",
    "    print(f'ncomp1: {ncomp1}')\n",
    "    # print(f'Mean Abs. Error: {mae1:.2f}')\n",
    "    print(f'R2-score: {r2score1:.2f}')\n",
    "    ## ocp \n",
    "    ## data\n",
    "    pca2 = PCA(n_components=ncomp_perc)  # Keep 95% of variance\n",
    "    X_train2_pca = pca2.fit_transform(X_train2)\n",
    "    X_test2_pca = pca2.transform(X_test2)    \n",
    "    ncomp2 = pca2.n_components_    \n",
    "    ## model train\n",
    "    lr, depth, n_est = 0.2, 8, 500\n",
    "    # model2 = xgb.XGBRegressor(learning_rate=lr, max_depth=depth, n_estimators=n_est)\n",
    "    model2 = LinearRegression()\n",
    "    model2.fit(X_train2_pca, y_train2)\n",
    "    ## model test\n",
    "    y_pred2 = model2.predict(X_test2_pca)\n",
    "    mae2 = mean_absolute_error(y_test2, y_pred2)\n",
    "    r2score2 = r2_score(y_test2, y_pred2)\n",
    "    ## evaluation\n",
    "    print(f'ncomp2: {ncomp2}')\n",
    "    # print(f'Mean Abs. Error: {mae2:.2f}')\n",
    "    print(f'R2-score: {r2score2:.2f}')\n",
    "    \n",
    "    #####################################################################################################\n",
    "    ## invariant\n",
    "    #####################################################################################################    \n",
    "    print('-- invariant --')    \n",
    "\n",
    "    #############################################################\n",
    "    # Convert to PyTorch Tensors\n",
    "    train_data1 = TensorDataset(torch.tensor(X_train1, dtype=torch.float32), torch.tensor(y_train1, dtype=torch.float32))\n",
    "    train_data2 = TensorDataset(torch.tensor(X_train2, dtype=torch.float32), torch.tensor(y_train2, dtype=torch.float32))\n",
    "    # Create Data Loaders\n",
    "    batch_size = 10000  # Set your batch size\n",
    "    train_loader1 = DataLoader(train_data1, batch_size=batch_size, shuffle=True)\n",
    "    train_loader2 = DataLoader(train_data2, batch_size=batch_size, shuffle=True)    \n",
    "    ## build a InvariantModel to transform 'd' to 'pca(d)',\n",
    "    ## use that 'InvariantModel transformation' instead of pca\n",
    "    len_embedding, abstract_len_embedding = 1536, int((ncomp1+ncomp2)/2)    \n",
    "    model_invariant = get_model_invariant(\n",
    "        train_loader1, train_loader2, len_embedding, abstract_len_embedding, trial_no    \n",
    "    )\n",
    "    ## save the model\n",
    "    model_path = 'v3/model_invariant_' + str(exp_name) + '_' + str(trial_no) + '.pt'\n",
    "    torch.save(model_invariant.state_dict(), model_path)    \n",
    "    # load the model\n",
    "    model_invariant = DataParallel(InvariantModel(len_embedding, abstract_len_embedding))\n",
    "    model_invariant.load_state_dict(torch.load(model_path))\n",
    "    model_invariant = model_invariant.module\n",
    "    # model_invariant.eval()    \n",
    "    #############################################################\n",
    "    ## cathub     \n",
    "    ## data\n",
    "    X_train1_inv = model_invariant.encoder(torch.from_numpy(X_train1).float()).detach().numpy()\n",
    "    X_test1_inv = model_invariant.encoder(torch.from_numpy(X_test1).float()).detach().numpy()\n",
    "    ## model train\n",
    "    lr, depth, n_est = 0.2, 8, 500\n",
    "    # model1 = xgb.XGBRegressor(learning_rate=lr, max_depth=depth, n_estimators=n_est)\n",
    "    model1 = LinearRegression()\n",
    "    model1.fit(X_train1_inv, y_train1)\n",
    "    ## model test\n",
    "    y_pred1 = model1.predict(X_test1_inv)\n",
    "    mae1 = mean_absolute_error(y_test1, y_pred1)\n",
    "    r2score1 = r2_score(y_test1, y_pred1)\n",
    "    ## evaluation\n",
    "    # print(f'Mean Abs. Error: {mae1:.2f}')\n",
    "    print(f'R2-score: {r2score1:.2f}')\n",
    "    ## ocp \n",
    "    ## data\n",
    "    X_train2_inv = model_invariant.encoder(torch.from_numpy(X_train2).float()).detach().numpy()\n",
    "    X_test2_inv = model_invariant.encoder(torch.from_numpy(X_test2).float()).detach().numpy()\n",
    "    ## model train\n",
    "    lr, depth, n_est = 0.2, 8, 500\n",
    "    # model2 = xgb.XGBRegressor(learning_rate=lr, max_depth=depth, n_estimators=n_est)\n",
    "    model2 = LinearRegression()\n",
    "    model2.fit(X_train2_inv, y_train2)\n",
    "    ## model test\n",
    "    y_pred2 = model2.predict(X_test2_inv)\n",
    "    mae2 = mean_absolute_error(y_test2, y_pred2)\n",
    "    r2score2 = r2_score(y_test2, y_pred2)\n",
    "    ## evaluation\n",
    "    # print(f'Mean Abs. Error: {mae2:.2f}')\n",
    "    print(f'R2-score: {r2score2:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ce2d61-a345-48d6-8c69-145b417cd146",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0134b50f-d2be-41a2-9d69-85776e77acb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45c9e1e-335b-448b-86bd-2b86db65bf18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

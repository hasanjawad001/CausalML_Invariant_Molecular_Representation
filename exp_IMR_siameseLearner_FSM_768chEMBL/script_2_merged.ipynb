{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The nltk version is 3.4.5.\n",
      "The scikit-learn version is 1.0.2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at mrm8488/chEMBL26_smiles_v2 were not used when initializing RobertaModel: ['lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46, 769)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at mrm8488/chEMBL26_smiles_v2 were not used when initializing RobertaModel: ['lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46, 769)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at mrm8488/chEMBL26_smiles_v2 were not used when initializing RobertaModel: ['lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46, 769)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at mrm8488/chEMBL26_smiles_v2 were not used when initializing RobertaModel: ['lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46, 769)\n",
      "(46, 769)\n",
      "(46, 769)\n",
      "(46, 769)\n",
      "(46, 769)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=412627)\u001b[0m  /home/mchowdh5/.local/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning:Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.545e-03, tolerance: 4.294e-04\n",
      "\u001b[2m\u001b[36m(pid=412627)\u001b[0m  /home/mchowdh5/.local/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning:Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.928e-03, tolerance: 5.233e-04\n",
      "\u001b[2m\u001b[36m(pid=412627)\u001b[0m  /home/mchowdh5/.local/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning:Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.326e-02, tolerance: 5.233e-04\n",
      "\u001b[2m\u001b[36m(pid=412627)\u001b[0m  /home/mchowdh5/.local/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning:Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.212e-03, tolerance: 5.233e-04\n",
      "\u001b[2m\u001b[36m(pid=412627)\u001b[0m  /home/mchowdh5/.local/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning:Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.156e-02, tolerance: 7.345e-04\n",
      "\u001b[2m\u001b[36m(pid=412627)\u001b[0m  /home/mchowdh5/.local/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning:Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.810e-04, tolerance: 5.551e-04\n",
      "\u001b[2m\u001b[36m(pid=412627)\u001b[0m  /home/mchowdh5/.local/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning:Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.052e-02, tolerance: 5.551e-04\n"
     ]
    }
   ],
   "source": [
    "## 10 trials, \n",
    "## 6 envs,\n",
    "## 3 models (regular, PCA, Siamese) ## no SiameseIRM since specific env is used only\n",
    "## tasks:\n",
    "##     1. seperate feature target for 6 envs (6 files in v10/datasets/)\n",
    "##     2. use utils for 2a and 2b- \n",
    "##        2a. take the SMILES notation and convert it by \"Chemical_SMILES_to_Flat_Fingerprint_RingStructs.ipynb -> convertNewSmilesToOldSmiles\" \n",
    "##            there are two cells which has the \"convertNewSmilesToOldSmiles\" function, use the first one (do not use the later cell's function)\n",
    "##        2b. after converting use \"Chemical_SMILES_to_Fingerprint.ipynb -> readSmilesToFingerprints\" function to get the fingerprints\n",
    "##     3. use 10 trials, 6 envs, 3 models to evaluate the performance\n",
    "##     4. ----------USE SIAMESE NN ALMOST SIMILAR TO PCA\n",
    "## extra:\n",
    "##     1. estimator PCA from version 1.0.2 (saved object) when using version 0.24.2 (here)z;\n",
    "##        1a. import nltk, sklearn\n",
    "##        1b. !pip install -U scikit-learn==1.0.2\n",
    "##        1c. print('The nltk version is {}.'.format(nltk.__version__))\n",
    "##        1d. print('The scikit-learn version is {}.'.format(sklearn.__version__))\n",
    "## extra:\n",
    "##     1. train with 768(24) pretrained model (abstract len emb depends on PCA(768(24)) on 90%(99%)?)\n",
    "##     *2. use all validation model (with struct_code 0 (simple) and 1 (complex))\n",
    "##     *3. use v8 (0- simple/PCA) struct\n",
    "##     *4. use v7 (1- complex) struct\n",
    "##     5. use v9 (simp+comp, solo env, 768)\n",
    "##     6. use v10 (simp+comp, solo env, 24)\n",
    "##     7. use v11 (simp+comp, beef env, 768)\n",
    "\n",
    "## imports\n",
    "\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel, pipeline, AutoModelForMaskedLM\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import grad\n",
    "from torch.utils.data import DataLoader    \n",
    "import torchvision.utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge, RidgeCV, LassoCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import stats\n",
    "import random\n",
    "# from pytorch_lightning.core.lightning import LightningModule\n",
    "# from pytorch_lightning import Trainer\n",
    "# import pytorch_lightning as pl\n",
    "# from ray_lightning import RayPlugin, RayShardedPlugin\n",
    "import ray\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import PIL.ImageOps\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "from tqdm.auto import tqdm\n",
    "import pickle as pk\n",
    "import time\n",
    "import utils\n",
    "from sklearn.decomposition import PCA\n",
    "import math\n",
    "from pprint import pprint\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "import nltk, sklearn\n",
    "print('The nltk version is {}.'.format(nltk.__version__))\n",
    "print('The scikit-learn version is {}.'.format(sklearn.__version__))\n",
    "\n",
    "## classes\n",
    "class SiameseNetwork(torch.nn.Module):\n",
    "    def __init__(self, len_embedding, abstract_len_embedding, use_irm=False, n_hidden_node=32, struct_code=0):\n",
    "        '''\n",
    "            struct_code {0=simple, 1=comple} structure\n",
    "        '''\n",
    "        \n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.loss = nn.L1Loss(reduction=\"mean\") \n",
    "        self.use_irm = use_irm\n",
    "        self.len_embedding = len_embedding\n",
    "        self.abstract_len_embedding = abstract_len_embedding  \n",
    "        self.n_hidden_node = n_hidden_node\n",
    "        #-----------change_1\n",
    "        if struct_code == 0:\n",
    "            self.nn_reg = nn.Sequential(\n",
    "                nn.Linear(self.len_embedding, self.abstract_len_embedding),\n",
    "            )\n",
    "        elif struct_code == 1:\n",
    "            self.nn_reg = nn.Sequential(\n",
    "                nn.Linear(self.len_embedding, self.n_hidden_node),nn.ReLU(inplace=True),nn.BatchNorm1d(self.n_hidden_node),            \n",
    "                nn.Linear(self.n_hidden_node, int(self.n_hidden_node/4)),nn.ReLU(inplace=True),nn.BatchNorm1d(int(self.n_hidden_node/4)),nn.Dropout(p=0.2),\n",
    "                nn.Linear(int(self.n_hidden_node/4), self.abstract_len_embedding),\n",
    "            )\n",
    "        else:\n",
    "            self.nn_reg = nn.Sequential(\n",
    "                nn.Linear(self.len_embedding, self.abstract_len_embedding),\n",
    "            )            \n",
    "        #-----------change_1\n",
    "        self.nn_final_reg = nn.Sequential(\n",
    "            nn.Linear(self.abstract_len_embedding * 2, self.n_hidden_node),nn.ReLU(inplace=True),nn.BatchNorm1d(self.n_hidden_node),\n",
    "            nn.Linear(self.n_hidden_node, int(self.n_hidden_node/4)),nn.ReLU(inplace=True),nn.BatchNorm1d(int(self.n_hidden_node/4)),nn.Dropout(p=0.2),\n",
    "            nn.Linear(int(self.n_hidden_node/4), 1),\n",
    "        )\n",
    "\n",
    "    def forward_reg(self, x):\n",
    "        output = self.nn_reg(x)\n",
    "        return output\n",
    "\n",
    "    def forward_final_reg(self, x):\n",
    "        output = self.nn_final_reg(x)\n",
    "        return output\n",
    "\n",
    "    def forward(self, fp1, fp2):\n",
    "        a = self.forward_reg(fp1)\n",
    "        b = self.forward_reg(fp2)\n",
    "        x = torch.cat([a, b], dim=1)  # hstack\n",
    "        output = self.forward_final_reg(x)\n",
    "        return output\n",
    "\n",
    "    def compute_penalty(self, losses, dummy_w):\n",
    "        g = grad(losses, dummy_w, create_graph=True)[0]\n",
    "        r = g.pow(2)\n",
    "        return r     \n",
    "\n",
    "## functions\n",
    "\n",
    "def get_secondary_env(env=None):\n",
    "    x_e, y_e = env[0].numpy(), env[1].numpy()\n",
    "\n",
    "    list_primary_feature, list_primary_target = [], []\n",
    "    list_secondary_feature, list_secondary_target = [], []\n",
    "\n",
    "    for i in range(x_e.shape[0]):\n",
    "        list_primary_feature.append(x_e[i])\n",
    "        list_primary_target.append(y_e[i])\n",
    "        i += 1\n",
    "\n",
    "    for i in range(len(list_primary_feature)):\n",
    "        for j in range(len(list_primary_feature)):\n",
    "            if i == j: ## n * (n-1) combinations\n",
    "                pass\n",
    "            else:\n",
    "                a = list_primary_feature[i]\n",
    "                b = list_primary_feature[j]\n",
    "                sf = np.hstack((a, b))\n",
    "                st = list_primary_target[i] - list_primary_target[j]\n",
    "                list_secondary_feature.append(sf)\n",
    "                list_secondary_target.append(st)\n",
    "    array_secondary_feature = np.array(list_secondary_feature, dtype='float32')\n",
    "    array_secondary_target = np.array(list_secondary_target, dtype='float32').reshape((-1, 1))\n",
    "    senv = torch.from_numpy(array_secondary_feature), torch.from_numpy(array_secondary_target)\n",
    "    return senv\n",
    "\n",
    "#### 1. seperate feature target for 6 envs (6 files in v10/datasets/)\n",
    "\n",
    "dir_datasets = 'datasets/' ## 'datasets/v2/'\n",
    "\n",
    "df_1_energy = pd.read_excel(dir_datasets + 'file_1.xlsx', sheet_name=0) ## energy sheet\n",
    "m_1_energy = df_1_energy.values[1:]\n",
    "m_1_name = m_1_energy[:,[0]]\n",
    "m_1_smiles = m_1_energy[:,[1]]\n",
    "m_1_energy_envs = m_1_energy[:,[2, 3, 4, 5]]\n",
    "for env_no in range(m_1_energy_envs.shape[1]):\n",
    "    m_1_name_smiles = np.hstack((m_1_name, m_1_smiles))\n",
    "    feature_target = np.hstack((m_1_name_smiles, m_1_energy_envs[:, [env_no]]))\n",
    "    np.savetxt(dir_datasets + 'smiles_vs_energy_env_' + str(env_no) + '.csv', feature_target, delimiter = \",\", fmt='%s')\n",
    "\n",
    "## funtion to generate pretrained embedding from SMILE notations\n",
    "\n",
    "## change_2 : change in v10(24) from v9(768) to accomodate manual 24 length embedding \n",
    "def get_embed_from_smiles(list_species_smiles, emb_style='auto_768'):\n",
    "    if emb_style=='manual_24':\n",
    "        list_emb = []\n",
    "        for i, s1 in enumerate(smiles):\n",
    "            try:\n",
    "                s2 = utils.convertNewSmilesToOldSmiles(s1)\n",
    "                s3 = utils.readSmilesToFingerprints(s2)\n",
    "            except Exception as e:\n",
    "                print(env_no, i, e)\n",
    "                s3 = ['-'] * 24\n",
    "            list_emb.append(s3)    \n",
    "        return np.array(list_emb)\n",
    "    else:\n",
    "        ## --------------- this is commented out in the server ----------------------- ##\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/chEMBL26_smiles_v2\")\n",
    "        model = AutoModel.from_pretrained(\"mrm8488/chEMBL26_smiles_v2\")\n",
    "        # Load the model into the GPU if avilabile and switch to inference mode\n",
    "        fe = pipeline('feature-extraction', model=model, tokenizer=tokenizer,device=0)\n",
    "        sequences_Example = list_species_smiles\n",
    "        embedding_1 = fe(sequences_Example)\n",
    "        n1, n2, n3 = len(embedding_1), len(embedding_1[0]), len(embedding_1[0][0])\n",
    "        embedding_2 = np.zeros((n1, n3))\n",
    "        list_embedding = []\n",
    "        for i, e1 in enumerate(embedding_1):\n",
    "            e2 = np.array(e1).mean(axis=0)        \n",
    "            embedding_2[i, :] = e2\n",
    "        embedding_2 = np.array(embedding_2)                    \n",
    "        return embedding_2\n",
    "\n",
    "dir_datasets = 'datasets/'\n",
    "for env_no in range(4): \n",
    "    file_name = dir_datasets + 'smiles_vs_energy_env_' + str(env_no) + '.csv'\n",
    "    df = pd.read_csv(file_name, sep=\",\", header=None) \n",
    "    m = df.values\n",
    "    smiles = [row[1] for row in m] ## 1 for smiles ==> (row[0],row[1],row[2]) == (name, smiles, energy)\n",
    "    \n",
    "    array_emb = get_embed_from_smiles(smiles, emb_style='auto_768') ## change_2 : change in v10(24) from v9(768) to accomodate manual 24 length embedding     \n",
    "    feature_target = np.hstack((array_emb, m[:,[2]])) ## index 2 used here for energy\n",
    "    \n",
    "    print(feature_target.shape)\n",
    "    np.savetxt('datasets/embedding_vs_energy_env_' + str(env_no) + '.csv', feature_target, delimiter = \",\", fmt='%s')\n",
    "#     print()\n",
    "\n",
    "## change_2 : change in v10(24) from v9(768) to accomodate manual 24 length embedding \n",
    "## skip invalid samples => index in each env [41,41,41,41,'-',9] for manual_24\n",
    "## skip invalid samples => index in each env ['-','-','-','-','-','-'] for auto_768\n",
    "list_skip_index = ['-','-','-','-']\n",
    "dir_datasets = 'datasets/'\n",
    "for env_no in range(4): \n",
    "    data = genfromtxt(dir_datasets + 'embedding_vs_energy_env_' + str(env_no) + '.csv', delimiter=',', dtype='float32')\n",
    "    skip_index = list_skip_index[env_no]\n",
    "    if skip_index == '-':\n",
    "        feature = data[:, 0:-1]\n",
    "        target = data[:, [-1]]\n",
    "    elif type(skip_index)== type(0):\n",
    "        feature = np.vstack((data[:skip_index, 0:-1], data[skip_index+1:, 0:-1]))\n",
    "        target = np.vstack((data[:skip_index, [-1]], data[skip_index+1:, [-1]]))\n",
    "    else:\n",
    "        print('Error!!!')\n",
    "    feature_target = np.hstack((feature, target))\n",
    "    print(feature_target.shape)\n",
    "    np.savetxt('datasets/filtered_embedding_vs_energy_env_' + str(env_no) + '.csv', feature_target, delimiter = \",\", fmt='%s')\n",
    "\n",
    "ray.shutdown()\n",
    "ray.init(ignore_reinit_error=True, num_cpus=12) ## detects automatically: num_cpus=64\n",
    "\n",
    "## functions\n",
    "\n",
    "def get_best_models(\n",
    "    len_embedding=None, abstract_len_embedding=None, num_iterations=None, \n",
    "    X_train=None, y_train=None, trial_no=None, env_no=None\n",
    "):\n",
    "    \n",
    "    env = torch.from_numpy(X_train), torch.from_numpy(y_train)\n",
    "    senv = get_secondary_env(env=env)\n",
    "    x_env, y_env = senv\n",
    "#     n = int(x_env.shape[0])\n",
    "#     if env_no == 5:\n",
    "#         n_train = int(n * 0.67) ##\n",
    "#     else:\n",
    "#         n_train = int(n * 0.8) ## train: validation == 80:20\n",
    "#     p = torch.randperm(n)\n",
    "#     p_train, p_test = p[0:n_train], p[n_train:]\n",
    "#     X_train, X_test, y_train, y_test = x_env[p_train], x_env[p_test], y_env[p_train], y_env[p_test]   \n",
    "    \n",
    "    \n",
    "    ## model siamese\n",
    "    list_model_info = []\n",
    "    best_model_info, best_loss_siamese = (None, None, None, None), math.inf\n",
    "    #-----------change_1    \n",
    "    for _n_hidden_node in [128]: ## hint: select something that is divisible by 4  ## [512,256,128,64,32,16,8]\n",
    "    #-----------change_1\n",
    "        for _lr in [1e-4, 1e-3, 1e-2, 1e-1]: ## [1e-4,1e-3,1e-2,1e-1,1]\n",
    "            for _struct_code in [0]: ## [0,1]\n",
    "                model_siamese = SiameseNetwork(\n",
    "                    len_embedding, abstract_len_embedding, use_irm=False, n_hidden_node=_n_hidden_node,\n",
    "                    struct_code=_struct_code\n",
    "                )\n",
    "                optimizer_siamese = torch.optim.Adam(model_siamese.parameters(), lr=_lr)\n",
    "\n",
    "                loss_siamese = None\n",
    "                for epoch in range(num_iterations):\n",
    "                    error_siamese = 0\n",
    "#                     for x, y in [(X_train, y_train)]:\n",
    "                    for x, y in [(x_env, y_env)]:                      \n",
    "                        p = torch.randperm(len(x))\n",
    "                        x_e = x[p]\n",
    "                        y_e = y[p]\n",
    "                        fp1 = x_e[:, list(range(0, len_embedding, 1))]\n",
    "                        fp2 = x_e[:, list(range(len_embedding, 2 * len_embedding, 1))]\n",
    "                        y_pred_siamese = model_siamese(fp1, fp2) \n",
    "                        error_e_siamese = model_siamese.loss(y_pred_siamese, y_e)\n",
    "                        error_siamese += error_e_siamese\n",
    "\n",
    "                    loss_siamese = 1 * error_siamese \n",
    "\n",
    "                    optimizer_siamese.zero_grad() ## clear buffer   \n",
    "                    loss_siamese.backward() ## calculate gradient for all params\n",
    "                    optimizer_siamese.step() ## update parameters using calculated gradients\n",
    "\n",
    "                ## validation\n",
    "#                 test_fp1 = X_test[:, list(range(0, len_embedding, 1))]\n",
    "#                 test_fp2 = X_test[:, list(range(len_embedding, 2 * len_embedding, 1))]\n",
    "#                 test_y_pred_siamese = model_siamese(test_fp1, test_fp2)\n",
    "#                 test_loss_siamese = model_siamese.loss(test_y_pred_siamese, y_test)\n",
    "#                 if test_loss_siamese < best_loss_siamese:\n",
    "#                     best_model_info = (_n_hidden_node, _lr, _struct_code, model_siamese) \n",
    "#                     best_loss_siamese = test_loss_siamese\n",
    "                if loss_siamese < best_loss_siamese:\n",
    "                    best_model_info = (_n_hidden_node, _lr, _struct_code, model_siamese)\n",
    "                    best_loss_siamese = loss_siamese\n",
    "                file1 = open('logger.log', 'a+')  \n",
    "                file1.writelines(\n",
    "                    'trial_no, env_no, n_hidden_node, lr, struct_code, loss_siamese, best_loss_siamese: ' \\\n",
    "                    + str(trial_no) + str(env_no) + str(_n_hidden_node) + str(_lr) + str(_struct_code) \\\n",
    "                    + str(loss_siamese) + str(best_loss_siamese) + '\\n\\n'\n",
    "                )\n",
    "                file1.close()\n",
    "                model_info = (_n_hidden_node, _lr, _struct_code, model_siamese)\n",
    "                list_model_info.append(model_info)\n",
    "             \n",
    "    return best_model_info, list_model_info        \n",
    "\n",
    "\n",
    "\n",
    "@ray.remote(num_returns=1)\n",
    "def get_result(\n",
    "    st, trial_no, list_env, dir_datasets, should_standardize, \n",
    "    len_embedding, num_iterations,  pca_n_percent\n",
    "):\n",
    "    for env_no in list_env:\n",
    "        d_result = {}\n",
    "        data = genfromtxt(dir_datasets + 'filtered_embedding_vs_energy_env_' + str(env_no) + '.csv', delimiter=',', dtype='float32')\n",
    "        feature = data[:, 0:-1]\n",
    "        target = data[:, [-1]]\n",
    "        if should_standardize:\n",
    "            scaler = StandardScaler().fit(feature)\n",
    "            feature = scaler.transform(feature)\n",
    "            \n",
    "        #####################################################################################################################################            \n",
    "        ## model 1: regular\n",
    "        model_name = 'regular' ## use the embeddings as it is\n",
    "        feature_regular, target_regular = feature, target\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            feature_regular, target_regular, test_size=0.33, random_state=trial_no\n",
    "        )        \n",
    "        ##\n",
    "        trainX, testX, trainY, testY = X_train, X_test, y_train.ravel(), y_test.ravel()\n",
    "        mlrun = utils.MLPredsByCV(cross_validation_split_no = 5)        \n",
    "        for alg in ['ridge','lasso','elastic','krr','svr']: ## ['ridge','lasso','elastic','krr','svr','gp']:\n",
    "            if alg == 'svr':\n",
    "                errors = mlrun.SVR_CV(trainX, testX, trainY, testY)\n",
    "            elif alg == 'krr':\n",
    "                errors = mlrun.KRR_CV(trainX, testX, trainY, testY)\n",
    "            elif alg == 'ridge':\n",
    "                errors = mlrun.Ridge_CV(trainX, testX, trainY, testY)\n",
    "            elif alg == 'lasso':\n",
    "                errors = mlrun.Lasso_CV(trainX, testX, trainY, testY)\n",
    "            elif alg == 'elastic':\n",
    "                errors = mlrun.Elastic_CV(trainX, testX, trainY, testY)\n",
    "            error = np.mean(errors)\n",
    "            key = (str(trial_no), str(env_no), str(model_name), str(alg))\n",
    "            d_result[key] = error\n",
    "        file1 = open('logger.log', 'a+')  \n",
    "        file1.writelines(\n",
    "            'regular: time, trial_no, env_no, model_name: ' \\\n",
    "            + str(time.time()-st) + '=>  ' + str(trial_no) + ', ' \\\n",
    "            + str(env_no) + ', ' + str(model_name) + '\\n\\n'\n",
    "        )\n",
    "        file1.close()            \n",
    "        \n",
    "        #####################################################################################################################################    \n",
    "        ## model 2: PCA\n",
    "        model_name = 'PCA' ## use the transformed embeddings by PCA\n",
    "        pca_dump_name = dir_datasets + 'pca_' + str(env_no) + '.pkl'\n",
    "        if should_standardize:\n",
    "            pca_dump_name = dir_datasets + 'pca_std_' + str(env_no) + '.pkl'                    \n",
    "        #-----------change_1\n",
    "        pca = PCA(n_components=pca_n_percent) ## e.g. 24 => 6 or other number of components\n",
    "        feature_train, feature_test, target_train, target_test = train_test_split(\n",
    "            feature, target, test_size=0.33, random_state=trial_no\n",
    "        )    \n",
    "        pca.fit(feature_train)\n",
    "        ## pickle dump\n",
    "        pk.dump(pca, open(pca_dump_name,\"wb\"))\n",
    "        ## later reload the pickle file\n",
    "        time.sleep(trial_no*8)                   \n",
    "        pca = pk.load(open(pca_dump_name,\"rb\"))\n",
    "        #-----------change_1\n",
    "        pca_n_components = pca.n_components_\n",
    "        file1 = open('logger.log', 'a+')  \n",
    "        file1.writelines('trial_no, env_no, pca_n_components: ' + str(trial_no) + ', ' + str(env_no) + ', ' + str(pca_n_components) + '\\n\\n')\n",
    "        file1.close()            \n",
    "\n",
    "        feature_pca, target_pca = pca.transform(feature), target\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            feature_pca, target_pca, test_size=0.33, random_state=trial_no\n",
    "        )    \n",
    "        ##\n",
    "        trainX, testX, trainY, testY = X_train, X_test, y_train.ravel(), y_test.ravel()\n",
    "        mlrun = utils.MLPredsByCV(cross_validation_split_no = 5)                \n",
    "        for alg in ['ridge','lasso','elastic','krr','svr']: ## ['ridge','lasso','elastic','krr','svr','gp']:\n",
    "            if alg == 'svr':\n",
    "                errors = mlrun.SVR_CV(trainX, testX, trainY, testY)\n",
    "            elif alg == 'krr':\n",
    "                errors = mlrun.KRR_CV(trainX, testX, trainY, testY)\n",
    "            elif alg == 'ridge':\n",
    "                errors = mlrun.Ridge_CV(trainX, testX, trainY, testY)\n",
    "            elif alg == 'lasso':\n",
    "                errors = mlrun.Lasso_CV(trainX, testX, trainY, testY)\n",
    "            elif alg == 'elastic':\n",
    "                errors = mlrun.Elastic_CV(trainX, testX, trainY, testY)\n",
    "            error = np.mean(errors)\n",
    "            key = (str(trial_no), str(env_no), str(model_name), str(alg))\n",
    "            d_result[key] = error\n",
    "        file1 = open('logger.log', 'a+')  \n",
    "        file1.writelines(\n",
    "            'pca: time, trial_no, env_no, model_name: ' \\\n",
    "            + str(time.time()-st) + '=>  ' + str(trial_no) + ', ' \\\n",
    "            + str(env_no) + ', ' + str(model_name) + '\\n\\n'\n",
    "        )\n",
    "        file1.close()            \n",
    "            \n",
    "        #####################################################################################################################################    \n",
    "        ## model 3: Siamese\n",
    "        pca_dump_name = dir_datasets + 'pca_' + str(env_no) + '.pkl'\n",
    "        if should_standardize:\n",
    "            pca_dump_name = dir_datasets + 'pca_std_' + str(env_no) + '.pkl'   \n",
    "        time.sleep(trial_no*12)                       \n",
    "        pca = pk.load(open(pca_dump_name,\"rb\"))\n",
    "        pca_n_components = pca.n_components_\n",
    "        #-----------------------------------------------------------------------------------\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            feature, target, test_size=0.33, random_state=trial_no\n",
    "        )           \n",
    "        best_model_info, list_model_info = get_best_models(\n",
    "            len_embedding=len_embedding, abstract_len_embedding=pca_n_components,\n",
    "            num_iterations=num_iterations, X_train=X_train, y_train=y_train,\n",
    "            trial_no=trial_no, env_no=env_no\n",
    "        )\n",
    "        # for model_info in list_model_info:  ## change_3\n",
    "        for model_info in [best_model_info]:          \n",
    "            _nhn, _lr, _sc, model_siamese = model_info[0], model_info[1], model_info[2], model_info[3]  \n",
    "            \n",
    "            # model_name = 'Siamese_' + str(_nhn) + '_' + str(_lr) + '_' + str(_sc) ## use the transformed embeddings by Siamese\n",
    "            model_name = 'Siamese' ## change_3\n",
    "            feature_siamese, target_siamese = model_siamese.forward_reg(torch.from_numpy(feature)).detach().numpy(), target \n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                feature_siamese, target_siamese, test_size=0.33, random_state=trial_no\n",
    "            )   \n",
    "            \n",
    "        #-----------------------------------------------------------------------------------        \n",
    "        #\n",
    "            trainX, testX, trainY, testY = X_train, X_test, y_train.ravel(), y_test.ravel()\n",
    "            mlrun = utils.MLPredsByCV(cross_validation_split_no = 5)                \n",
    "            for alg in ['ridge','lasso','elastic','krr','svr']: ## ['ridge','lasso','elastic','krr','svr','gp']:\n",
    "                if alg == 'svr':\n",
    "                    errors = mlrun.SVR_CV(trainX, testX, trainY, testY)\n",
    "                elif alg == 'krr':\n",
    "                    errors = mlrun.KRR_CV(trainX, testX, trainY, testY)\n",
    "                elif alg == 'ridge':\n",
    "                    errors = mlrun.Ridge_CV(trainX, testX, trainY, testY)\n",
    "                elif alg == 'lasso':\n",
    "                    errors = mlrun.Lasso_CV(trainX, testX, trainY, testY)\n",
    "                elif alg == 'elastic':\n",
    "                    errors = mlrun.Elastic_CV(trainX, testX, trainY, testY)\n",
    "                error = np.mean(errors)\n",
    "                key = (str(trial_no), str(env_no), str(model_name), str(alg))\n",
    "                d_result[key] = error\n",
    "            file1 = open('logger.log', 'a+')  \n",
    "            file1.writelines('siamese: time, trial_no, env_no, model_name: ' + str(time.time()-st) + '=>  ' + str(trial_no) + ', ' + str(env_no) + ', ' + str(model_name) + '\\n\\n')\n",
    "            file1.close()            \n",
    "\n",
    "        #####################################################################################################################################     \n",
    "        ##\n",
    "        \n",
    "        with open('datasets/d_result_' + str(trial_no) + '_' + str(env_no) + '.pickle', 'wb') as handle: \n",
    "            pk.dump(d_result, handle, protocol=pk.HIGHEST_PROTOCOL)\n",
    "    return 0\n",
    "\n",
    "if __name__=='__main__':\n",
    "    #-----------change_1\n",
    "    num_trials = 10 ## should be 10\n",
    "    list_env = [0,1,2,3] ## should be [0,1,2,3,4,5]\n",
    "    num_iterations = 2 ## should be (500 or 1000) for train on BEEF (extrapolation) or (5000/500 or 10000/1000) for train on individual env (intrapolation)\n",
    "    #-----------change_1\n",
    "    #\n",
    "    dir_datasets = 'datasets/'\n",
    "    should_standardize = False\n",
    "    len_embedding=768 ## change_2\n",
    "    pca_n_percent=0.90 ## change_2\n",
    "\n",
    "    ## main_1: get_result\n",
    "\n",
    "    st = time.time()\n",
    "    list_result_id = []\n",
    "    for trial_no in range(num_trials):\n",
    "        result_id = get_result.remote(\n",
    "            st, trial_no, list_env, dir_datasets, should_standardize, \n",
    "            len_embedding, num_iterations, pca_n_percent\n",
    "        )\n",
    "        list_result_id.append(result_id)\n",
    "    list_result = ray.get(list_result_id)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The nltk version is 3.6.7.\n",
      "The scikit-learn version is 1.0.2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at mrm8488/chEMBL26_smiles_v2 were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46, 769)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at mrm8488/chEMBL26_smiles_v2 were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46, 769)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at mrm8488/chEMBL26_smiles_v2 were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46, 769)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at mrm8488/chEMBL26_smiles_v2 were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46, 769)\n",
      "(46, 769)\n",
      "(46, 769)\n",
      "(46, 769)\n",
      "(46, 769)\n",
      "(46, 6)\n",
      "(46, 768)\n",
      "(46, 2000)\n",
      "(46, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " pid=13172)\u001b[0m  C:\\Users\\mchowdh5\\Anaconda3\\envs\\env_model_energy_predictor\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning:Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.823e-03, tolerance: 4.294e-04\n",
      " pid=13172)\u001b[0m  C:\\Users\\mchowdh5\\Anaconda3\\envs\\env_model_energy_predictor\\lib\\site-packages\\ray\\workers/default_worker.py:296: UserWarning:The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:189.)\n",
      " pid=5368)\u001b[0m  C:\\Users\\mchowdh5\\Anaconda3\\envs\\env_model_energy_predictor\\lib\\site-packages\\ray\\workers/default_worker.py:296: UserWarning:The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:189.)\n",
      " pid=15528)\u001b[0m  C:\\Users\\mchowdh5\\Anaconda3\\envs\\env_model_energy_predictor\\lib\\site-packages\\ray\\workers/default_worker.py:296: UserWarning:The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:189.)\n",
      " pid=7624)\u001b[0m  C:\\Users\\mchowdh5\\Anaconda3\\envs\\env_model_energy_predictor\\lib\\site-packages\\ray\\workers/default_worker.py:296: UserWarning:The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:189.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " pid=13172)\u001b[0m trial_no, env_no, time:  0 0 100.51818656921387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " pid=13172)\u001b[0m  C:\\Users\\mchowdh5\\Anaconda3\\envs\\env_model_energy_predictor\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning:Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.900e-03, tolerance: 5.233e-04\n",
      " pid=13172)\u001b[0m  C:\\Users\\mchowdh5\\Anaconda3\\envs\\env_model_energy_predictor\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning:Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.465e-03, tolerance: 5.233e-04\n",
      " pid=26212)\u001b[0m  C:\\Users\\mchowdh5\\Anaconda3\\envs\\env_model_energy_predictor\\lib\\site-packages\\ray\\workers/default_worker.py:296: UserWarning:The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:189.)\n",
      " pid=2904)\u001b[0m  C:\\Users\\mchowdh5\\Anaconda3\\envs\\env_model_energy_predictor\\lib\\site-packages\\ray\\workers/default_worker.py:296: UserWarning:The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:189.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " pid=5368)\u001b[0m trial_no, env_no, time:  1 0 136.27443170547485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " pid=25960)\u001b[0m  C:\\Users\\mchowdh5\\Anaconda3\\envs\\env_model_energy_predictor\\lib\\site-packages\\ray\\workers/default_worker.py:296: UserWarning:The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:189.)\n",
      " pid=24108)\u001b[0m  C:\\Users\\mchowdh5\\Anaconda3\\envs\\env_model_energy_predictor\\lib\\site-packages\\ray\\workers/default_worker.py:296: UserWarning:The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:189.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " pid=15528)\u001b[0m trial_no, env_no, time:  2 0 185.16852688789368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " pid=19168)\u001b[0m  C:\\Users\\mchowdh5\\Anaconda3\\envs\\env_model_energy_predictor\\lib\\site-packages\\ray\\workers/default_worker.py:296: UserWarning:The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:189.)\n",
      " pid=8400)\u001b[0m  C:\\Users\\mchowdh5\\Anaconda3\\envs\\env_model_energy_predictor\\lib\\site-packages\\ray\\workers/default_worker.py:296: UserWarning:The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:189.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " pid=7624)\u001b[0m trial_no, env_no, time:  3 0 254.88501024246216\n",
      " pid=13172)\u001b[0m trial_no, env_no, time:  0 1 282.50118684768677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " pid=13172)\u001b[0m  C:\\Users\\mchowdh5\\Anaconda3\\envs\\env_model_energy_predictor\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning:Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.705e-03, tolerance: 7.345e-04\n",
      " pid=13172)\u001b[0m  C:\\Users\\mchowdh5\\Anaconda3\\envs\\env_model_energy_predictor\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning:Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.235e-03, tolerance: 7.345e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " pid=26212)\u001b[0m trial_no, env_no, time:  4 0 286.5918321609497\n",
      " pid=2904)\u001b[0m trial_no, env_no, time:  5 0 308.14509105682373\n",
      " pid=25960)\u001b[0m trial_no, env_no, time:  6 0 327.6499767303467\n",
      " pid=5368)\u001b[0m trial_no, env_no, time:  1 1 344.41981172561646\n",
      " pid=24108)\u001b[0m trial_no, env_no, time:  7 0 346.6113271713257\n",
      " pid=19168)\u001b[0m trial_no, env_no, time:  8 0 358.9005358219147\n",
      " pid=8400)\u001b[0m trial_no, env_no, time:  9 0 367.30020093917847\n",
      " pid=15528)\u001b[0m trial_no, env_no, time:  2 1 375.5747609138489\n",
      " pid=13172)\u001b[0m trial_no, env_no, time:  0 2 400.70242166519165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " pid=13172)\u001b[0m  C:\\Users\\mchowdh5\\Anaconda3\\envs\\env_model_energy_predictor\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning:Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.608e-03, tolerance: 5.551e-04\n",
      " pid=13172)\u001b[0m  C:\\Users\\mchowdh5\\Anaconda3\\envs\\env_model_energy_predictor\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning:Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.011e-02, tolerance: 5.551e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " pid=7624)\u001b[0m trial_no, env_no, time:  3 1 417.7516438961029\n",
      " pid=5368)\u001b[0m trial_no, env_no, time:  1 2 470.08406376838684\n",
      " pid=26212)\u001b[0m trial_no, env_no, time:  4 1 476.1094126701355\n",
      " pid=13172)\u001b[0m trial_no, env_no, time:  0 3 509.1870994567871\n",
      " pid=2904)\u001b[0m trial_no, env_no, time:  5 1 528.561155796051\n",
      " pid=15528)\u001b[0m trial_no, env_no, time:  2 2 535.2677783966064\n",
      " pid=25960)\u001b[0m trial_no, env_no, time:  6 1 569.4761297702789\n",
      " pid=7624)\u001b[0m trial_no, env_no, time:  3 2 597.00474858284\n",
      " pid=24108)\u001b[0m trial_no, env_no, time:  7 1 610.8358526229858\n",
      " pid=5368)\u001b[0m trial_no, env_no, time:  1 3 615.8578772544861\n",
      " pid=19168)\u001b[0m trial_no, env_no, time:  8 1 635.8746366500854\n",
      " pid=8400)\u001b[0m trial_no, env_no, time:  9 1 659.142819404602\n",
      " pid=26212)\u001b[0m trial_no, env_no, time:  4 2 664.7088422775269\n",
      " pid=15528)\u001b[0m trial_no, env_no, time:  2 3 675.5406773090363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " pid=2904)\u001b[0m  C:\\Users\\mchowdh5\\Anaconda3\\envs\\env_model_energy_predictor\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning:Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.689e-01, tolerance: 5.631e-02\n",
      " pid=2904)\u001b[0m  C:\\Users\\mchowdh5\\Anaconda3\\envs\\env_model_energy_predictor\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning:Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.861e-01, tolerance: 6.599e-02\n",
      " pid=2904)\u001b[0m  C:\\Users\\mchowdh5\\Anaconda3\\envs\\env_model_energy_predictor\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning:Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.548e-01, tolerance: 4.559e-02\n",
      " pid=2904)\u001b[0m  C:\\Users\\mchowdh5\\Anaconda3\\envs\\env_model_energy_predictor\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning:Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.468e-01, tolerance: 5.631e-02\n",
      " pid=2904)\u001b[0m  C:\\Users\\mchowdh5\\Anaconda3\\envs\\env_model_energy_predictor\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning:Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.586e-01, tolerance: 4.733e-02\n",
      " pid=2904)\u001b[0m  C:\\Users\\mchowdh5\\Anaconda3\\envs\\env_model_energy_predictor\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning:Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.527e-01, tolerance: 5.091e-02\n",
      " pid=2904)\u001b[0m  C:\\Users\\mchowdh5\\Anaconda3\\envs\\env_model_energy_predictor\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning:Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.899e-01, tolerance: 6.599e-02\n",
      " pid=2904)\u001b[0m  C:\\Users\\mchowdh5\\Anaconda3\\envs\\env_model_energy_predictor\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning:Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.297e-01, tolerance: 4.559e-02\n",
      " pid=2904)\u001b[0m  C:\\Users\\mchowdh5\\Anaconda3\\envs\\env_model_energy_predictor\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning:Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.293e-01, tolerance: 5.631e-02\n",
      " pid=2904)\u001b[0m  C:\\Users\\mchowdh5\\Anaconda3\\envs\\env_model_energy_predictor\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning:Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.728e-01, tolerance: 4.733e-02\n",
      " pid=2904)\u001b[0m  C:\\Users\\mchowdh5\\Anaconda3\\envs\\env_model_energy_predictor\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning:Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.493e-02, tolerance: 5.091e-02\n",
      " pid=2904)\u001b[0m  C:\\Users\\mchowdh5\\Anaconda3\\envs\\env_model_energy_predictor\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning:Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.250e-01, tolerance: 6.599e-02\n",
      " pid=2904)\u001b[0m  C:\\Users\\mchowdh5\\Anaconda3\\envs\\env_model_energy_predictor\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning:Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.609e-01, tolerance: 4.559e-02\n",
      " pid=2904)\u001b[0m  C:\\Users\\mchowdh5\\Anaconda3\\envs\\env_model_energy_predictor\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning:Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.408e-01, tolerance: 5.631e-02\n",
      " pid=2904)\u001b[0m  C:\\Users\\mchowdh5\\Anaconda3\\envs\\env_model_energy_predictor\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning:Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.796e-01, tolerance: 4.733e-02\n",
      " pid=2904)\u001b[0m  C:\\Users\\mchowdh5\\Anaconda3\\envs\\env_model_energy_predictor\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning:Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.667e-01, tolerance: 6.599e-02\n",
      " pid=2904)\u001b[0m  C:\\Users\\mchowdh5\\Anaconda3\\envs\\env_model_energy_predictor\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning:Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.192e-01, tolerance: 4.559e-02\n",
      " pid=2904)\u001b[0m  C:\\Users\\mchowdh5\\Anaconda3\\envs\\env_model_energy_predictor\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning:Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.724e-01, tolerance: 5.631e-02\n",
      " pid=2904)\u001b[0m  C:\\Users\\mchowdh5\\Anaconda3\\envs\\env_model_energy_predictor\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning:Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.281e-01, tolerance: 4.733e-02\n",
      " pid=2904)\u001b[0m  C:\\Users\\mchowdh5\\Anaconda3\\envs\\env_model_energy_predictor\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning:Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.143e-01, tolerance: 6.599e-02\n",
      " pid=2904)\u001b[0m  C:\\Users\\mchowdh5\\Anaconda3\\envs\\env_model_energy_predictor\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning:Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.201e-02, tolerance: 4.559e-02\n",
      " pid=2904)\u001b[0m  C:\\Users\\mchowdh5\\Anaconda3\\envs\\env_model_energy_predictor\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning:Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.197e-01, tolerance: 5.631e-02\n",
      " pid=2904)\u001b[0m  C:\\Users\\mchowdh5\\Anaconda3\\envs\\env_model_energy_predictor\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning:Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.525e-02, tolerance: 4.733e-02\n",
      " pid=2904)\u001b[0m  C:\\Users\\mchowdh5\\Anaconda3\\envs\\env_model_energy_predictor\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning:Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.668e-01, tolerance: 6.599e-02\n",
      " pid=2904)\u001b[0m  C:\\Users\\mchowdh5\\Anaconda3\\envs\\env_model_energy_predictor\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning:Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.308e-02, tolerance: 4.559e-02\n",
      " pid=2904)\u001b[0m  C:\\Users\\mchowdh5\\Anaconda3\\envs\\env_model_energy_predictor\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning:Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.766e-01, tolerance: 5.631e-02\n",
      " pid=2904)\u001b[0m  C:\\Users\\mchowdh5\\Anaconda3\\envs\\env_model_energy_predictor\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning:Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.461e-02, tolerance: 4.733e-02\n",
      " pid=2904)\u001b[0m  C:\\Users\\mchowdh5\\Anaconda3\\envs\\env_model_energy_predictor\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning:Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.238e-01, tolerance: 6.599e-02\n",
      " pid=2904)\u001b[0m  C:\\Users\\mchowdh5\\Anaconda3\\envs\\env_model_energy_predictor\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning:Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.961e-02, tolerance: 4.559e-02\n",
      " pid=2904)\u001b[0m  C:\\Users\\mchowdh5\\Anaconda3\\envs\\env_model_energy_predictor\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning:Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.415e-01, tolerance: 5.631e-02\n",
      " pid=2904)\u001b[0m  C:\\Users\\mchowdh5\\Anaconda3\\envs\\env_model_energy_predictor\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning:Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.980e-02, tolerance: 4.733e-02\n",
      " pid=2904)\u001b[0m  C:\\Users\\mchowdh5\\Anaconda3\\envs\\env_model_energy_predictor\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning:Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.846e-01, tolerance: 6.599e-02\n",
      " pid=2904)\u001b[0m  C:\\Users\\mchowdh5\\Anaconda3\\envs\\env_model_energy_predictor\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning:Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.974e-02, tolerance: 4.559e-02\n",
      " pid=2904)\u001b[0m  C:\\Users\\mchowdh5\\Anaconda3\\envs\\env_model_energy_predictor\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning:Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.126e-01, tolerance: 5.631e-02\n",
      " pid=2904)\u001b[0m  C:\\Users\\mchowdh5\\Anaconda3\\envs\\env_model_energy_predictor\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning:Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.898e-02, tolerance: 4.733e-02\n",
      " pid=2904)\u001b[0m  C:\\Users\\mchowdh5\\Anaconda3\\envs\\env_model_energy_predictor\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning:Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.488e-01, tolerance: 6.599e-02\n",
      " pid=2904)\u001b[0m  C:\\Users\\mchowdh5\\Anaconda3\\envs\\env_model_energy_predictor\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning:Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.892e-01, tolerance: 5.631e-02\n",
      " pid=2904)\u001b[0m  C:\\Users\\mchowdh5\\Anaconda3\\envs\\env_model_energy_predictor\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning:Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.161e-01, tolerance: 6.599e-02\n",
      " pid=2904)\u001b[0m  C:\\Users\\mchowdh5\\Anaconda3\\envs\\env_model_energy_predictor\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning:Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.718e-02, tolerance: 5.631e-02\n",
      " pid=2904)\u001b[0m  C:\\Users\\mchowdh5\\Anaconda3\\envs\\env_model_energy_predictor\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning:Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.340e-01, tolerance: 6.599e-02\n",
      " pid=2904)\u001b[0m  C:\\Users\\mchowdh5\\Anaconda3\\envs\\env_model_energy_predictor\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning:Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.391e-01, tolerance: 6.599e-02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " pid=2904)\u001b[0m trial_no, env_no, time:  5 2 704.6199605464935\n",
      " pid=7624)\u001b[0m trial_no, env_no, time:  3 3 721.2428598403931\n",
      " pid=25960)\u001b[0m trial_no, env_no, time:  6 2 746.9167113304138\n",
      " pid=26212)\u001b[0m trial_no, env_no, time:  4 3 800.1489953994751\n",
      " pid=24108)\u001b[0m trial_no, env_no, time:  7 2 807.8061516284943\n",
      " pid=19168)\u001b[0m trial_no, env_no, time:  8 2 858.0301530361176\n",
      " pid=2904)\u001b[0m trial_no, env_no, time:  5 3 865.4042477607727\n",
      " pid=8400)\u001b[0m trial_no, env_no, time:  9 2 897.6869139671326\n",
      " pid=25960)\u001b[0m trial_no, env_no, time:  6 3 918.4622857570648\n",
      " pid=24108)\u001b[0m trial_no, env_no, time:  7 3 987.6239066123962\n",
      " pid=19168)\u001b[0m trial_no, env_no, time:  8 3 1058.0065023899078\n",
      " pid=8400)\u001b[0m trial_no, env_no, time:  9 3 1117.3529744148254\n"
     ]
    }
   ],
   "source": [
    "## 10 trials, \n",
    "## 6 envs,\n",
    "## 3 models (regular, PCA, Siamese)\n",
    "## tasks:\n",
    "##     1. seperate feature target for 6 envs (6 files in v11/datasets/)\n",
    "##     2. use utils for 2a and 2b- \n",
    "##     2a. take the SMILES notation and convert it by \"Chemical_SMILES_to_Flat_Fingerprint_RingStructs.ipynb -> convertNewSmilesToOldSmiles\" \n",
    "##        there are two cells which has the \"convertNewSmilesToOldSmiles\" function, use the first one (do not use the later cell's function)\n",
    "##     2b. after converting use \"Chemical_SMILES_to_Fingerprint.ipynb -> readSmilesToFingerprints\" function to get the fingerprints\n",
    "##     3. use 10 trials, 6 envs, 3 models to evaluate the performance\n",
    "## extra:\n",
    "##     1. estimator PCA from version 1.0.2 (saved object) when using version 0.24.2 (here)z;\n",
    "##        1a. import nltk, sklearn\n",
    "##        1b. !pip install -U scikit-learn==1.0.2\n",
    "##        1c. print('The nltk version is {}.'.format(nltk.__version__))\n",
    "##        1d. print('The scikit-learn version is {}.'.format(sklearn.__version__))\n",
    "## extra:\n",
    "##     1. train with 768 pretrained model (abstract len emb depends on PCA(768) on 90%)\n",
    "##     *2. use all validation model (with struct_code 0 (simple) and 1 (complex))\n",
    "##     *3. use v8 (0- simple/PCA, solo, 768) struct\n",
    "##     *4. use v7 (1- complex, solo, 768) struct\n",
    "##     5. use v9 (simp+comp, solo env, 768)\n",
    "##     6. use v10 (simp+comp, solo env, 24)\n",
    "##     7. use v11 (simp+comp, beef env, 768)\n",
    "\n",
    "## imports\n",
    "\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel, pipeline, AutoModelForMaskedLM\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import grad\n",
    "from torch.utils.data import DataLoader    \n",
    "import torchvision.utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge, RidgeCV, LassoCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import stats\n",
    "import random\n",
    "# from pytorch_lightning.core.lightning import LightningModule\n",
    "# from pytorch_lightning import Trainer\n",
    "# import pytorch_lightning as pl\n",
    "# from ray_lightning import RayPlugin, RayShardedPlugin\n",
    "import ray\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import PIL.ImageOps\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "from tqdm.auto import tqdm\n",
    "import pickle as pk\n",
    "import time\n",
    "import utils\n",
    "from sklearn.decomposition import PCA\n",
    "import math\n",
    "from pprint import pprint\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "import nltk, sklearn\n",
    "print('The nltk version is {}.'.format(nltk.__version__))\n",
    "print('The scikit-learn version is {}.'.format(sklearn.__version__))\n",
    "\n",
    "## classes\n",
    "class SiameseNetwork(torch.nn.Module):\n",
    "    def __init__(self, len_embedding, abstract_len_embedding, use_irm=False, n_hidden_node=32, struct_code=0):\n",
    "        '''\n",
    "            struct_code {0=simple, 1=comple} structure\n",
    "        '''\n",
    "        \n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.loss = nn.L1Loss(reduction=\"mean\") \n",
    "        self.use_irm = use_irm\n",
    "        self.len_embedding = len_embedding\n",
    "        self.abstract_len_embedding = abstract_len_embedding  \n",
    "        self.n_hidden_node = n_hidden_node\n",
    "        #-----------change_1\n",
    "        if struct_code == 0:\n",
    "            self.nn_reg = nn.Sequential(\n",
    "                nn.Linear(self.len_embedding, self.abstract_len_embedding),\n",
    "            )\n",
    "        elif struct_code == 1:\n",
    "            self.nn_reg = nn.Sequential(\n",
    "                nn.Linear(self.len_embedding, self.n_hidden_node),nn.ReLU(inplace=True),nn.BatchNorm1d(self.n_hidden_node),            \n",
    "                nn.Linear(self.n_hidden_node, int(self.n_hidden_node/4)),nn.ReLU(inplace=True),nn.BatchNorm1d(int(self.n_hidden_node/4)),nn.Dropout(p=0.2),\n",
    "                nn.Linear(int(self.n_hidden_node/4), self.abstract_len_embedding),\n",
    "            )\n",
    "        else:\n",
    "            self.nn_reg = nn.Sequential(\n",
    "                nn.Linear(self.len_embedding, self.abstract_len_embedding),\n",
    "            )            \n",
    "        #-----------change_1\n",
    "        self.nn_final_reg = nn.Sequential(\n",
    "            nn.Linear(self.abstract_len_embedding * 2, self.n_hidden_node),nn.ReLU(inplace=True),nn.BatchNorm1d(self.n_hidden_node),\n",
    "            nn.Linear(self.n_hidden_node, int(self.n_hidden_node/4)),nn.ReLU(inplace=True),nn.BatchNorm1d(int(self.n_hidden_node/4)),nn.Dropout(p=0.2),\n",
    "            nn.Linear(int(self.n_hidden_node/4), 1),\n",
    "        )\n",
    "\n",
    "    def forward_reg(self, x):\n",
    "        output = self.nn_reg(x)\n",
    "        return output\n",
    "\n",
    "    def forward_final_reg(self, x):\n",
    "        output = self.nn_final_reg(x)\n",
    "        return output\n",
    "\n",
    "    def forward(self, fp1, fp2):\n",
    "        a = self.forward_reg(fp1)\n",
    "        b = self.forward_reg(fp2)\n",
    "        x = torch.cat([a, b], dim=1)  # hstack\n",
    "        output = self.forward_final_reg(x)\n",
    "        return output\n",
    "\n",
    "    def compute_penalty(self, losses, dummy_w):\n",
    "        g = grad(losses, dummy_w, create_graph=True)[0]\n",
    "        r = g.pow(2)\n",
    "        return r     \n",
    "\n",
    "## functions\n",
    "\n",
    "def get_secondary_env(env=None):\n",
    "    x_e, y_e = env[0].numpy(), env[1].numpy()\n",
    "\n",
    "    list_primary_feature, list_primary_target = [], []\n",
    "    list_secondary_feature, list_secondary_target = [], []\n",
    "\n",
    "    for i in range(x_e.shape[0]):\n",
    "        list_primary_feature.append(x_e[i])\n",
    "        list_primary_target.append(y_e[i])\n",
    "        i += 1\n",
    "\n",
    "    for i in range(len(list_primary_feature)):\n",
    "        for j in range(len(list_primary_feature)):\n",
    "            if i == j:\n",
    "                pass\n",
    "            else:\n",
    "                a = list_primary_feature[i]\n",
    "                b = list_primary_feature[j]\n",
    "                sf = np.hstack((a, b))\n",
    "                st = list_primary_target[i] - list_primary_target[j]\n",
    "                list_secondary_feature.append(sf)\n",
    "                list_secondary_target.append(st)\n",
    "    array_secondary_feature = np.array(list_secondary_feature, dtype='float32')\n",
    "    array_secondary_target = np.array(list_secondary_target, dtype='float32').reshape((-1, 1))\n",
    "    senv = torch.from_numpy(array_secondary_feature), torch.from_numpy(array_secondary_target)\n",
    "    return senv\n",
    "\n",
    "dir_datasets = 'datasets/' ## 'datasets/v2/'\n",
    "\n",
    "df_1_energy = pd.read_excel(dir_datasets + 'file_1.xlsx', sheet_name=0) ## energy sheet\n",
    "m_1_energy = df_1_energy.values[1:]\n",
    "m_1_name = m_1_energy[:,[0]]\n",
    "m_1_smiles = m_1_energy[:,[1]]\n",
    "m_1_energy_envs = m_1_energy[:,[2, 3, 4, 5]]\n",
    "for env_no in range(m_1_energy_envs.shape[1]):\n",
    "    m_1_name_smiles = np.hstack((m_1_name, m_1_smiles))\n",
    "    feature_target = np.hstack((m_1_name_smiles, m_1_energy_envs[:, [env_no]]))\n",
    "    np.savetxt(dir_datasets + 'smiles_vs_energy_env_' + str(env_no) + '.csv', feature_target, delimiter = \",\", fmt='%s')\n",
    "\n",
    "\n",
    "\n",
    "## funtion to generate pretrained embedding from SMILE notations\n",
    "\n",
    "## change_2 : change in v10(24) from v9(768) to accomodate manual 24 length embedding \n",
    "def get_embed_from_smiles(list_species_smiles, emb_style='auto_768'):\n",
    "    if emb_style=='manual_24':\n",
    "        list_emb = []\n",
    "        for i, s1 in enumerate(smiles):\n",
    "            try:\n",
    "                s2 = utils.convertNewSmilesToOldSmiles(s1)\n",
    "                s3 = utils.readSmilesToFingerprints(s2)\n",
    "            except Exception as e:\n",
    "                print(env_no, i, e)\n",
    "                s3 = ['-'] * 24\n",
    "            list_emb.append(s3)    \n",
    "        return np.array(list_emb)\n",
    "    else:\n",
    "        ## --------------- this is commented out in the server ----------------------- ##\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/chEMBL26_smiles_v2\")\n",
    "        model = AutoModel.from_pretrained(\"mrm8488/chEMBL26_smiles_v2\")\n",
    "        # Load the model into the GPU if avilabile and switch to inference mode\n",
    "        fe = pipeline('feature-extraction', model=model, tokenizer=tokenizer,device=0)\n",
    "        sequences_Example = list_species_smiles\n",
    "        embedding_1 = fe(sequences_Example)\n",
    "        n1, n2, n3, n4 = len(embedding_1), len(embedding_1[0]), len(embedding_1[0][0]), len(embedding_1[0][0][0])\n",
    "        embedding_2 = np.zeros((n1, n4))\n",
    "        list_embedding = []\n",
    "        for i, e1 in enumerate(embedding_1):\n",
    "            e2 = np.array(e1[0]).mean(axis=0)        \n",
    "            embedding_2[i, :] = e2\n",
    "        embedding_2 = np.array(embedding_2)                    \n",
    "        return embedding_2\n",
    "\n",
    "dir_datasets = 'datasets/'\n",
    "for env_no in range(4): \n",
    "    file_name = dir_datasets + 'smiles_vs_energy_env_' + str(env_no) + '.csv'\n",
    "    df = pd.read_csv(file_name, sep=\",\", header=None) \n",
    "    m = df.values\n",
    "    smiles = [row[1] for row in m] ## 1 for smiles ==> (row[0],row[1],row[2]) == (name, smiles, energy)\n",
    "    \n",
    "    array_emb = get_embed_from_smiles(smiles, emb_style='auto_768') ## change_2 : change in v10(24) from v9(768) to accomodate manual 24 length embedding     \n",
    "    feature_target = np.hstack((array_emb, m[:,[2]])) ## index 2 used here for energy\n",
    "    \n",
    "    print(feature_target.shape)\n",
    "    np.savetxt('datasets/embedding_vs_energy_env_' + str(env_no) + '.csv', feature_target, delimiter = \",\", fmt='%s')\n",
    "#     print()\n",
    "\n",
    "## change_2 : change in v10(24) from v9(768) to accomodate manual 24 length embedding \n",
    "## skip invalid samples => index in each env [41,41,41,41,'-',9] for manual_24\n",
    "## skip invalid samples => index in each env ['-','-','-','-','-','-'] for auto_768\n",
    "list_skip_index = ['-','-','-','-']\n",
    "dir_datasets = 'datasets/'\n",
    "for env_no in range(4): \n",
    "    data = genfromtxt(dir_datasets + 'embedding_vs_energy_env_' + str(env_no) + '.csv', delimiter=',', dtype='float32')\n",
    "    skip_index = list_skip_index[env_no]\n",
    "    if skip_index == '-':\n",
    "        feature = data[:, 0:-1]\n",
    "        target = data[:, [-1]]\n",
    "    elif type(skip_index)== type(0):\n",
    "        feature = np.vstack((data[:skip_index, 0:-1], data[skip_index+1:, 0:-1]))\n",
    "        target = np.vstack((data[:skip_index, [-1]], data[skip_index+1:, [-1]]))\n",
    "    else:\n",
    "        print('Error!!!')\n",
    "    feature_target = np.hstack((feature, target))\n",
    "    print(feature_target.shape)\n",
    "    np.savetxt('datasets/filtered_embedding_vs_energy_env_' + str(env_no) + '.csv', feature_target, delimiter = \",\", fmt='%s')\n",
    "\n",
    "## reading data for Siamese\n",
    "\n",
    "dir_datasets = 'datasets/'\n",
    "#\n",
    "df_1_energy = pd.read_excel(dir_datasets + 'file_1.xlsx', sheet_name=0) ## energy sheet\n",
    "df_1_feature = genfromtxt(dir_datasets + 'embedding_vs_energy_env_0.csv', delimiter=',', dtype='float32')\n",
    "df_3_energy = pd.read_excel(dir_datasets + 'file_2.xlsx', sheet_name=0, engine='openpyxl') # pt111\n",
    "df_3_feature = genfromtxt(dir_datasets + 'embedding_vs_energy_env_0.csv', delimiter=',', dtype='float32')\n",
    "#\n",
    "m_1_energy = df_1_energy.values[1:]\n",
    "m_1_feature = df_1_feature[:,list(range(0,768))]\n",
    "m_3_energy = df_3_energy.values.T\n",
    "m_3_feature = df_3_feature[:,list(range(0,768))]\n",
    "#\n",
    "print(m_1_energy.shape)\n",
    "print(m_1_feature.shape)\n",
    "print(m_3_energy.shape)\n",
    "print(m_3_feature.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ray.shutdown()\n",
    "ray.init(ignore_reinit_error=True, num_cpus=16) ## detects automatically: num_cpus=64\n",
    "\n",
    "## functions\n",
    "\n",
    "def get_best_models(\n",
    "    num_sample_envs=None, m_1_feature=None, m_1_energy=None, m_3_energy=None, skip_index_beef=None,\n",
    "    len_embedding=None, abstract_len_embedding=None, num_iterations=None, random_state=None\n",
    "):\n",
    "    list_index = list(range(2000))\n",
    "    if random_state is None:\n",
    "        random.shuffle(list_index)\n",
    "    else:\n",
    "        random.Random(random_state).shuffle(list_index)\n",
    "    list_sample_index = list_index[0: num_sample_envs]\n",
    "    \n",
    "    ## train: target-feature\n",
    "    data_feature_train = m_1_feature\n",
    "    data_target_train_core = np.array(m_1_energy[:,[3]], dtype='float32')\n",
    "    data_target_train = np.zeros((data_target_train_core.shape[0], num_sample_envs))\n",
    "    for i in range(data_target_train_core.shape[0]):\n",
    "        for j in range(num_sample_envs):\n",
    "            sample_index = list_sample_index[j]\n",
    "            mean = data_target_train_core[i][0]\n",
    "            deviation = m_3_energy[i][sample_index]\n",
    "            value = mean + deviation\n",
    "            data_target_train[i][j] = value\n",
    "    if skip_index_beef is not None:\n",
    "        data_feature_train = np.vstack((data_feature_train[:skip_index_beef, :], data_feature_train[skip_index_beef+1:, :]))\n",
    "        data_target_train = np.vstack((data_target_train[:skip_index_beef, :], data_target_train[skip_index_beef+1:, :]))  \n",
    "    #     print(data_feature_train.shape, data_target_train.shape)    \n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    ## environments: primary\n",
    "    environments = []\n",
    "    for i in range(data_target_train.shape[1]):\n",
    "        env = torch.from_numpy(data_feature_train[0:]), torch.from_numpy(data_target_train[0:, [i]])\n",
    "        environments.append(env)\n",
    "#     print(len(environments), environments[0][0].shape, environments[0][1].shape)\n",
    "    \n",
    "    # print('==============================================================================================================================')\n",
    "    # print(len(environments))\n",
    "    # for i, env in enumerate(environments):\n",
    "    #     features, targets = env\n",
    "    #     print(features.shape, targets.shape)\n",
    "    #     if torch.isnan(features).any():\n",
    "    #         print(f\"Environment {i} contains NaN values.\")\n",
    "    # print('==============================================================================================================================')\n",
    "    \n",
    "    \n",
    "    ## environments: secondary\n",
    "    senvironments = []\n",
    "    for env in environments:\n",
    "        senv = get_secondary_env(env=env)\n",
    "        senvironments.append(senv)\n",
    "#     print(len(senvironments), senvironments[0][0].shape, senvironments[0][1].shape)  \n",
    "    \n",
    "    ## model siamese\n",
    "    list_model_info = []\n",
    "    best_model_info, best_loss_siamese = (None, None, None, None), math.inf\n",
    "    #-----------change_1    \n",
    "    for _n_hidden_node in [128]: ## hint: select something that is divisible by 4  \n",
    "    #-----------change_1\n",
    "        for _lr in [1e-4, 1e-3, 1e-2, 1e-1, 1]: \n",
    "            for _struct_code in [1]:\n",
    "                model_siamese = SiameseNetwork(\n",
    "                    len_embedding, abstract_len_embedding, use_irm=False, n_hidden_node=_n_hidden_node,\n",
    "                    struct_code=_struct_code\n",
    "                )\n",
    "                optimizer_siamese = torch.optim.Adam(model_siamese.parameters(), lr=_lr)\n",
    "                \n",
    "                loss_siamese = None\n",
    "                for epoch in range(num_iterations):\n",
    "                    error_siamese = 0\n",
    "                    for x, y in senvironments:\n",
    "                        p = torch.randperm(len(x))\n",
    "                        x_e = x[p]\n",
    "                        y_e = y[p]\n",
    "                        fp1 = x_e[:, list(range(0, len_embedding, 1))]\n",
    "                        fp2 = x_e[:, list(range(len_embedding, 2 * len_embedding, 1))]\n",
    "                        y_pred_siamese = model_siamese(fp1, fp2) \n",
    "                        error_e_siamese = model_siamese.loss(y_pred_siamese, y_e)\n",
    "                        error_siamese += error_e_siamese\n",
    "                        \n",
    "                    error_siamese = error_siamese/len(senvironments)\n",
    "                    loss_siamese = 1 * error_siamese \n",
    "\n",
    "                    optimizer_siamese.zero_grad() ## clear buffer   \n",
    "                    loss_siamese.backward() ## calculate gradient for all params\n",
    "                    optimizer_siamese.step() ## update parameters using calculated gradients\n",
    "                \n",
    "                if loss_siamese < best_loss_siamese:\n",
    "                    best_model_info = (_n_hidden_node, _lr, _struct_code, model_siamese)\n",
    "                    best_loss_siamese = loss_siamese\n",
    "                file1 = open('logger.log', 'a+')  \n",
    "                file1.writelines(\n",
    "                    'n_hidden_node, lr, struct_code, loss_siamese, best_loss_siamese: ' \\\n",
    "                    + str(_n_hidden_node) + str(_lr) + str(_struct_code) \\\n",
    "                    + str(loss_siamese) + str(best_loss_siamese) + '\\n\\n'\n",
    "                )\n",
    "                file1.close()                \n",
    "                model_info = (_n_hidden_node, _lr, _struct_code, model_siamese)\n",
    "                list_model_info.append(model_info)\n",
    "             \n",
    "    return best_model_info, list_model_info        \n",
    "\n",
    "@ray.remote(num_returns=1)\n",
    "def get_result(\n",
    "    st, trial_no, list_env, dir_datasets, should_standardize, \n",
    "    num_sample_envs, m_1_feature, m_1_energy, m_3_energy, skip_index_beef, \n",
    "    len_embedding, num_iterations, pca_n_percent\n",
    "):\n",
    "    for env_no in list_env:\n",
    "        d_result = {}        \n",
    "        data = genfromtxt(dir_datasets + 'filtered_embedding_vs_energy_env_' + str(env_no) + '.csv', delimiter=',', dtype='float32')\n",
    "        feature = data[:, 0:-1]\n",
    "        target = data[:, [-1]]\n",
    "        if should_standardize:\n",
    "            scaler = StandardScaler().fit(feature)\n",
    "            feature = scaler.transform(feature)\n",
    "            \n",
    "        #####################################################################################################################################            \n",
    "        ## model 1: regular\n",
    "        model_name = 'regular' ## use the embeddings as it is\n",
    "        feature_regular, target_regular = feature, target\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            feature_regular, target_regular, test_size=0.33, random_state=trial_no\n",
    "        )        \n",
    "        ##\n",
    "        trainX, testX, trainY, testY = X_train, X_test, y_train.ravel(), y_test.ravel()\n",
    "        mlrun = utils.MLPredsByCV(cross_validation_split_no = 5)   \n",
    "        for alg in ['ridge','lasso','elastic','krr','svr']: ## ['ridge','lasso','elastic','krr','svr','gp']:\n",
    "            if alg == 'svr':\n",
    "                errors = mlrun.SVR_CV(trainX, testX, trainY, testY)\n",
    "            elif alg == 'krr':\n",
    "                errors = mlrun.KRR_CV(trainX, testX, trainY, testY)\n",
    "            elif alg == 'ridge':\n",
    "                errors = mlrun.Ridge_CV(trainX, testX, trainY, testY)\n",
    "            elif alg == 'lasso':\n",
    "                errors = mlrun.Lasso_CV(trainX, testX, trainY, testY)\n",
    "            elif alg == 'elastic':\n",
    "                errors = mlrun.Elastic_CV(trainX, testX, trainY, testY)\n",
    "            error = np.mean(errors)\n",
    "            key = (str(trial_no), str(env_no), str(model_name), str(alg))\n",
    "            d_result[key] = error\n",
    "        file1 = open('logger.log', 'a+')  \n",
    "        file1.writelines(\n",
    "            'time, trial_no, env_no, model_name: ' \\\n",
    "            + str(time.time()-st) + '=>  ' + str(trial_no) + ', ' \\\n",
    "            + str(env_no) + ', ' + str(model_name) + '\\n\\n'\n",
    "        )\n",
    "        file1.close()            \n",
    "        ##################################################################################################################################### \n",
    "        ## model 2: PCA\n",
    "        model_name = 'PCA' ## use the transformed embeddings by PCA\n",
    "        pca_dump_name = dir_datasets + 'pca_' + str(env_no) + '.pkl'\n",
    "        if should_standardize:\n",
    "            pca_dump_name = dir_datasets + 'pca_std_' + str(env_no) + '.pkl'                    \n",
    "        #-----------change_1\n",
    "        pca = PCA(n_components=pca_n_percent) ## e.g. 24 => 6 or other number of components\n",
    "        feature_train, feature_test, target_train, target_test = train_test_split(\n",
    "            feature, target, test_size=0.33, random_state=trial_no\n",
    "        )    \n",
    "        pca.fit(feature_train)\n",
    "        ## pickle dump\n",
    "        pk.dump(pca, open(pca_dump_name,\"wb\"))\n",
    "        ## later reload the pickle file\n",
    "        time.sleep(trial_no*8)           \n",
    "        pca = pk.load(open(pca_dump_name,\"rb\"))\n",
    "        #-----------change_1\n",
    "        pca_n_components = pca.n_components_\n",
    "        file1 = open('logger.log', 'a+')  \n",
    "        file1.writelines(str(pca_n_components) + '\\n\\n')\n",
    "        file1.close()            \n",
    "\n",
    "        feature_pca, target_pca = pca.transform(feature), target\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            feature_pca, target_pca, test_size=0.33, random_state=trial_no\n",
    "        )    \n",
    "        ##\n",
    "        trainX, testX, trainY, testY = X_train, X_test, y_train.ravel(), y_test.ravel()\n",
    "        mlrun = utils.MLPredsByCV(cross_validation_split_no = 5)                \n",
    "        for alg in ['ridge','lasso','elastic','krr','svr']: ## ['ridge','lasso','elastic','krr','svr','gp']:\n",
    "            if alg == 'svr':\n",
    "                errors = mlrun.SVR_CV(trainX, testX, trainY, testY)\n",
    "            elif alg == 'krr':\n",
    "                errors = mlrun.KRR_CV(trainX, testX, trainY, testY)\n",
    "            elif alg == 'ridge':\n",
    "                errors = mlrun.Ridge_CV(trainX, testX, trainY, testY)\n",
    "            elif alg == 'lasso':\n",
    "                errors = mlrun.Lasso_CV(trainX, testX, trainY, testY)\n",
    "            elif alg == 'elastic':\n",
    "                errors = mlrun.Elastic_CV(trainX, testX, trainY, testY)\n",
    "            error = np.mean(errors)\n",
    "            key = (str(trial_no), str(env_no), str(model_name), str(alg))\n",
    "            d_result[key] = error\n",
    "        file1 = open('logger.log', 'a+')  \n",
    "        file1.writelines(\n",
    "            'time, trial_no, env_no, model_name: ' \\\n",
    "            + str(time.time()-st) + '=>  ' + str(trial_no) + ', ' \\\n",
    "            + str(env_no) + ', ' + str(model_name) + '\\n\\n'\n",
    "        )\n",
    "        file1.close()            \n",
    "            \n",
    "        #####################################################################################################################################    \n",
    "        ## model 3: Siamese\n",
    "        pca_dump_name = dir_datasets + 'pca_' + str(env_no) + '.pkl'\n",
    "        if should_standardize:\n",
    "            pca_dump_name = dir_datasets + 'pca_std_' + str(env_no) + '.pkl'   \n",
    "        time.sleep(trial_no*12)                       \n",
    "        pca = pk.load(open(pca_dump_name,\"rb\"))\n",
    "        pca_n_components = pca.n_components_\n",
    "        #-----------------------------------------------------------------------------------\n",
    "#         filename_best_model_info = 'best_model_info_' + str(trial_no)\n",
    "#         filename_list_model_info = 'list_model_info_' + str(trial_no)        \n",
    "#         try:\n",
    "#             with open(\"datasets/\" + filename_best_model_info + \".pickle\", 'rb') as handle:\n",
    "#                 best_model_info = pk.load(handle)\n",
    "#             with open(\"datasets/\" + filename_list_model_info + \".pickle\", 'rb') as handle:\n",
    "#                 list_model_info = pk.load(handle)\n",
    "                \n",
    "#             file1 = open('logger.log', 'a+')  \n",
    "#             file1.writelines(\n",
    "#                 'trial_no: ' + str(trial_no) + ', Yes, saved model found!' + '\\n\\n'\n",
    "#             )\n",
    "#             file1.close()                            \n",
    "#         except Exception as e: \n",
    "#             file1 = open('logger.log', 'a+')  \n",
    "#             file1.writelines(\n",
    "#                 'trial_no: ' + str(trial_no) + ', No saved model found: '+ str(e) + '\\n\\n'\n",
    "#             )\n",
    "#             file1.close()            \n",
    "            \n",
    "#             best_model_info, list_model_info = get_best_models(\n",
    "#                 num_sample_envs=num_sample_envs, m_1_feature=m_1_feature, m_1_energy=m_1_energy,\n",
    "#                 m_3_energy=m_3_energy, skip_index_beef=skip_index_beef, len_embedding=len_embedding, abstract_len_embedding=pca_n_components,\n",
    "#                 num_iterations=num_iterations\n",
    "#             )\n",
    "#             with open(\"datasets/\" + filename_best_model_info + \".pickle\", 'wb') as handle: \n",
    "#                 pk.dump(best_model_info, handle, protocol=pk.HIGHEST_PROTOCOL)\n",
    "#             with open(\"datasets/\" + filename_list_model_info + \".pickle\", 'wb') as handle: \n",
    "#                 pk.dump(list_model_info, handle, protocol=pk.HIGHEST_PROTOCOL)\n",
    "        best_model_info, list_model_info = get_best_models(\n",
    "            num_sample_envs=num_sample_envs, m_1_feature=m_1_feature, m_1_energy=m_1_energy,\n",
    "            m_3_energy=m_3_energy, skip_index_beef=skip_index_beef, len_embedding=len_embedding, abstract_len_embedding=pca_n_components,\n",
    "            num_iterations=num_iterations, random_state=trial_no\n",
    "        )                            \n",
    "    \n",
    "        for model_info in [best_model_info]:            \n",
    "            _nhn, _lr, _sc, model_siamese = model_info[0], model_info[1], model_info[2], model_info[3]  \n",
    "            \n",
    "            # model_name = 'Siamese_' + str(_nhn) + '_' + str(_lr) + '_' + str(_sc) ## use the transformed embeddings by Siamese\n",
    "            model_name = 'Siamese'            \n",
    "            feature_siamese, target_siamese = model_siamese.forward_reg(torch.from_numpy(feature)).detach().numpy(), target \n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                feature_siamese, target_siamese, test_size=0.33, random_state=trial_no\n",
    "            )    \n",
    "        #-----------------------------------------------------------------------------------        \n",
    "        #\n",
    "            trainX, testX, trainY, testY = X_train, X_test, y_train.ravel(), y_test.ravel()\n",
    "            mlrun = utils.MLPredsByCV(cross_validation_split_no = 5)                \n",
    "            for alg in ['ridge','lasso','elastic','krr','svr']: ## ['ridge','lasso','elastic','krr','svr','gp']:\n",
    "                if alg == 'svr':\n",
    "                    errors = mlrun.SVR_CV(trainX, testX, trainY, testY)\n",
    "                elif alg == 'krr':\n",
    "                    errors = mlrun.KRR_CV(trainX, testX, trainY, testY)\n",
    "                elif alg == 'ridge':\n",
    "                    errors = mlrun.Ridge_CV(trainX, testX, trainY, testY)\n",
    "                elif alg == 'lasso':\n",
    "                    errors = mlrun.Lasso_CV(trainX, testX, trainY, testY)\n",
    "                elif alg == 'elastic':\n",
    "                    errors = mlrun.Elastic_CV(trainX, testX, trainY, testY)\n",
    "                error = np.mean(errors)\n",
    "                key = (str(trial_no), str(env_no), str(model_name), str(alg))\n",
    "                d_result[key] = error\n",
    "            file1 = open('logger.log', 'a+')  \n",
    "            file1.writelines('siamese: time, trial_no, env_no, model_name: ' + str(time.time()-st) + '=>  ' + str(trial_no) + ', ' + str(env_no) + ', ' + str(model_name) + '\\n\\n')\n",
    "            file1.close()            \n",
    "\n",
    "        #####################################################################################################################################\n",
    "        ## model 4: SiameseIRM\n",
    "        #####################################################################################################################################     \n",
    "        print('trial_no, env_no, time: ', trial_no, env_no, time.time()-st)    \n",
    "        \n",
    "        with open('datasets/d_result_' + str(trial_no) + '_' + str(env_no) + '.pickle', 'wb') as handle: \n",
    "            pk.dump(d_result, handle, protocol=pk.HIGHEST_PROTOCOL)\n",
    "    return 0\n",
    "\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    num_trials = 10\n",
    "    list_env = [0,1,2,3] \n",
    "    num_iterations = 2 \n",
    "    ##\n",
    "    num_sample_envs = 50\n",
    "    dir_datasets = 'datasets/'\n",
    "    should_standardize = False\n",
    "    skip_index_beef = None ## 41\n",
    "    len_embedding=768 ## change_2\n",
    "    pca_n_percent=0.90 ## change_2\n",
    "\n",
    "    ## main_1\n",
    "\n",
    "    st = time.time()\n",
    "    list_result_id = []\n",
    "    for trial_no in range(num_trials):\n",
    "        result_id = get_result.remote(\n",
    "            st, trial_no, list_env, dir_datasets, should_standardize, \n",
    "            num_sample_envs, m_1_feature, m_1_energy, m_3_energy, skip_index_beef, \n",
    "            len_embedding, num_iterations, pca_n_percent\n",
    "        )\n",
    "        list_result_id.append(result_id)\n",
    "    list_result = ray.get(list_result_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

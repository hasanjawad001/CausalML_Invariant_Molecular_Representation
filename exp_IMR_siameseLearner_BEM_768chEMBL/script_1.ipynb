{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 10 trials, \n",
    "## 6 envs,\n",
    "## 3 models (regular, PCA, Siamese)\n",
    "## tasks:\n",
    "##     1. seperate feature target for 6 envs (6 files in v11/datasets/)\n",
    "##     2. use utils for 2a and 2b- \n",
    "##     2a. take the SMILES notation and convert it by \"Chemical_SMILES_to_Flat_Fingerprint_RingStructs.ipynb -> convertNewSmilesToOldSmiles\" \n",
    "##        there are two cells which has the \"convertNewSmilesToOldSmiles\" function, use the first one (do not use the later cell's function)\n",
    "##     2b. after converting use \"Chemical_SMILES_to_Fingerprint.ipynb -> readSmilesToFingerprints\" function to get the fingerprints\n",
    "##     3. use 10 trials, 6 envs, 3 models to evaluate the performance\n",
    "## extra:\n",
    "##     1. estimator PCA from version 1.0.2 (saved object) when using version 0.24.2 (here)z;\n",
    "##        1a. import nltk, sklearn\n",
    "##        1b. !pip install -U scikit-learn==1.0.2\n",
    "##        1c. print('The nltk version is {}.'.format(nltk.__version__))\n",
    "##        1d. print('The scikit-learn version is {}.'.format(sklearn.__version__))\n",
    "## extra:\n",
    "##     1. train with 768 pretrained model (abstract len emb depends on PCA(768) on 90%)\n",
    "##     *2. use all validation model (with struct_code 0 (simple) and 1 (complex))\n",
    "##     *3. use v8 (0- simple/PCA, solo, 768) struct\n",
    "##     *4. use v7 (1- complex, solo, 768) struct\n",
    "##     5. use v9 (simp+comp, solo env, 768)\n",
    "##     6. use v10 (simp+comp, solo env, 24)\n",
    "##     7. use v11 (simp+comp, beef env, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## imports\n",
    "\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel, pipeline, AutoModelForMaskedLM\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import grad\n",
    "from torch.utils.data import DataLoader    \n",
    "import torchvision.utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge, RidgeCV, LassoCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import stats\n",
    "import random\n",
    "# from pytorch_lightning.core.lightning import LightningModule\n",
    "# from pytorch_lightning import Trainer\n",
    "# import pytorch_lightning as pl\n",
    "# from ray_lightning import RayPlugin, RayShardedPlugin\n",
    "import ray\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import PIL.ImageOps\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "from tqdm.auto import tqdm\n",
    "import pickle as pk\n",
    "import time\n",
    "import utils\n",
    "from sklearn.decomposition import PCA\n",
    "import math\n",
    "from pprint import pprint\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The nltk version is 3.6.7.\n",
      "The scikit-learn version is 1.0.2.\n"
     ]
    }
   ],
   "source": [
    "import nltk, sklearn\n",
    "print('The nltk version is {}.'.format(nltk.__version__))\n",
    "print('The scikit-learn version is {}.'.format(sklearn.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## classes\n",
    "class SiameseNetwork(torch.nn.Module):\n",
    "    def __init__(self, len_embedding, abstract_len_embedding, use_irm=False, n_hidden_node=32, struct_code=0):\n",
    "        '''\n",
    "            struct_code {0=simple, 1=comple} structure\n",
    "        '''\n",
    "        \n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.loss = nn.L1Loss(reduction=\"mean\") \n",
    "        self.use_irm = use_irm\n",
    "        self.len_embedding = len_embedding\n",
    "        self.abstract_len_embedding = abstract_len_embedding  \n",
    "        self.n_hidden_node = n_hidden_node\n",
    "        #-----------change_1\n",
    "        if struct_code == 0:\n",
    "            self.nn_reg = nn.Sequential(\n",
    "                nn.Linear(self.len_embedding, self.abstract_len_embedding),\n",
    "            )\n",
    "        elif struct_code == 1:\n",
    "            self.nn_reg = nn.Sequential(\n",
    "                nn.Linear(self.len_embedding, self.n_hidden_node),nn.ReLU(inplace=True),nn.BatchNorm1d(self.n_hidden_node),            \n",
    "                nn.Linear(self.n_hidden_node, int(self.n_hidden_node/4)),nn.ReLU(inplace=True),nn.BatchNorm1d(int(self.n_hidden_node/4)),nn.Dropout(p=0.2),\n",
    "                nn.Linear(int(self.n_hidden_node/4), self.abstract_len_embedding),\n",
    "            )\n",
    "        else:\n",
    "            self.nn_reg = nn.Sequential(\n",
    "                nn.Linear(self.len_embedding, self.abstract_len_embedding),\n",
    "            )            \n",
    "        #-----------change_1\n",
    "        self.nn_final_reg = nn.Sequential(\n",
    "            nn.Linear(self.abstract_len_embedding * 2, self.n_hidden_node),nn.ReLU(inplace=True),nn.BatchNorm1d(self.n_hidden_node),\n",
    "            nn.Linear(self.n_hidden_node, int(self.n_hidden_node/4)),nn.ReLU(inplace=True),nn.BatchNorm1d(int(self.n_hidden_node/4)),nn.Dropout(p=0.2),\n",
    "            nn.Linear(int(self.n_hidden_node/4), 1),\n",
    "        )\n",
    "\n",
    "    def forward_reg(self, x):\n",
    "        output = self.nn_reg(x)\n",
    "        return output\n",
    "\n",
    "    def forward_final_reg(self, x):\n",
    "        output = self.nn_final_reg(x)\n",
    "        return output\n",
    "\n",
    "    def forward(self, fp1, fp2):\n",
    "        a = self.forward_reg(fp1)\n",
    "        b = self.forward_reg(fp2)\n",
    "        x = torch.cat([a, b], dim=1)  # hstack\n",
    "        output = self.forward_final_reg(x)\n",
    "        return output\n",
    "\n",
    "    def compute_penalty(self, losses, dummy_w):\n",
    "        g = grad(losses, dummy_w, create_graph=True)[0]\n",
    "        r = g.pow(2)\n",
    "        return r     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## functions\n",
    "\n",
    "def get_secondary_env(env=None):\n",
    "    x_e, y_e = env[0].numpy(), env[1].numpy()\n",
    "\n",
    "    list_primary_feature, list_primary_target = [], []\n",
    "    list_secondary_feature, list_secondary_target = [], []\n",
    "\n",
    "    for i in range(x_e.shape[0]):\n",
    "        list_primary_feature.append(x_e[i])\n",
    "        list_primary_target.append(y_e[i])\n",
    "        i += 1\n",
    "\n",
    "    for i in range(len(list_primary_feature)):\n",
    "        for j in range(len(list_primary_feature)):\n",
    "            if i == j:\n",
    "                pass\n",
    "            else:\n",
    "                a = list_primary_feature[i]\n",
    "                b = list_primary_feature[j]\n",
    "                sf = np.hstack((a, b))\n",
    "                st = list_primary_target[i] - list_primary_target[j]\n",
    "                list_secondary_feature.append(sf)\n",
    "                list_secondary_target.append(st)\n",
    "    array_secondary_feature = np.array(list_secondary_feature, dtype='float32')\n",
    "    array_secondary_target = np.array(list_secondary_target, dtype='float32').reshape((-1, 1))\n",
    "    senv = torch.from_numpy(array_secondary_feature), torch.from_numpy(array_secondary_target)\n",
    "    return senv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_datasets = 'datasets/' ## 'datasets/v2/'\n",
    "\n",
    "df_1_energy = pd.read_excel(dir_datasets + 'file_1.xlsx', sheet_name=0) ## energy sheet\n",
    "m_1_energy = df_1_energy.values[1:]\n",
    "m_1_name = m_1_energy[:,[0]]\n",
    "m_1_smiles = m_1_energy[:,[1]]\n",
    "m_1_energy_envs = m_1_energy[:,[2, 3, 4, 5]]\n",
    "for env_no in range(m_1_energy_envs.shape[1]):\n",
    "    m_1_name_smiles = np.hstack((m_1_name, m_1_smiles))\n",
    "    feature_target = np.hstack((m_1_name_smiles, m_1_energy_envs[:, [env_no]]))\n",
    "    np.savetxt(dir_datasets + 'smiles_vs_energy_env_' + str(env_no) + '.csv', feature_target, delimiter = \",\", fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## funtion to generate pretrained embedding from SMILE notations\n",
    "\n",
    "## change_2 : change in v10(24) from v9(768) to accomodate manual 24 length embedding \n",
    "def get_embed_from_smiles(list_species_smiles, emb_style='auto_768'):\n",
    "    if emb_style=='manual_24':\n",
    "        list_emb = []\n",
    "        for i, s1 in enumerate(smiles):\n",
    "            try:\n",
    "                s2 = utils.convertNewSmilesToOldSmiles(s1)\n",
    "                s3 = utils.readSmilesToFingerprints(s2)\n",
    "            except Exception as e:\n",
    "                print(env_no, i, e)\n",
    "                s3 = ['-'] * 24\n",
    "            list_emb.append(s3)    \n",
    "        return np.array(list_emb)\n",
    "    else:\n",
    "        ## --------------- this is commented out in the server ----------------------- ##\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/chEMBL26_smiles_v2\")\n",
    "        model = AutoModel.from_pretrained(\"mrm8488/chEMBL26_smiles_v2\")\n",
    "        # Load the model into the GPU if avilabile and switch to inference mode\n",
    "        fe = pipeline('feature-extraction', model=model, tokenizer=tokenizer,device=0)\n",
    "        sequences_Example = list_species_smiles\n",
    "        embedding_1 = fe(sequences_Example)\n",
    "        n1, n2, n3, n4 = len(embedding_1), len(embedding_1[0]), len(embedding_1[0][0]), len(embedding_1[0][0][0])\n",
    "        embedding_2 = np.zeros((n1, n4))\n",
    "        list_embedding = []\n",
    "        for i, e1 in enumerate(embedding_1):\n",
    "            e2 = np.array(e1[0]).mean(axis=0)        \n",
    "            embedding_2[i, :] = e2\n",
    "        embedding_2 = np.array(embedding_2)                    \n",
    "        return embedding_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 41 invalid literal for int() with base 10: '-'\n",
      "(46, 25)\n",
      "1 41 invalid literal for int() with base 10: '-'\n",
      "(46, 25)\n",
      "2 41 invalid literal for int() with base 10: '-'\n",
      "(46, 25)\n",
      "3 41 invalid literal for int() with base 10: '-'\n",
      "(46, 25)\n"
     ]
    }
   ],
   "source": [
    "dir_datasets = 'datasets/'\n",
    "for env_no in range(4): \n",
    "    file_name = dir_datasets + 'smiles_vs_energy_env_' + str(env_no) + '.csv'\n",
    "    df = pd.read_csv(file_name, sep=\",\", header=None) \n",
    "    m = df.values\n",
    "    smiles = [row[1] for row in m] ## 1 for smiles ==> (row[0],row[1],row[2]) == (name, smiles, energy)\n",
    "    \n",
    "    array_emb = get_embed_from_smiles(smiles, emb_style='auto_768') ## change_2 : change in v10(24) from v9(768) to accomodate manual 24 length embedding     \n",
    "    feature_target = np.hstack((array_emb, m[:,[2]])) ## index 2 used here for energy\n",
    "    \n",
    "    print(feature_target.shape)\n",
    "    np.savetxt('datasets/embedding_vs_energy_env_' + str(env_no) + '.csv', feature_target, delimiter = \",\", fmt='%s')\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45, 25)\n",
      "(45, 25)\n",
      "(45, 25)\n",
      "(45, 25)\n"
     ]
    }
   ],
   "source": [
    "## change_2 : change in v10(24) from v9(768) to accomodate manual 24 length embedding \n",
    "## skip invalid samples => index in each env [41,41,41,41,'-',9] for manual_24\n",
    "## skip invalid samples => index in each env ['-','-','-','-','-','-'] for auto_768\n",
    "list_skip_index = ['-','-','-','-']\n",
    "dir_datasets = 'datasets/'\n",
    "for env_no in range(4): \n",
    "    data = genfromtxt(dir_datasets + 'embedding_vs_energy_env_' + str(env_no) + '.csv', delimiter=',', dtype='float32')\n",
    "    skip_index = list_skip_index[env_no]\n",
    "    if skip_index == '-':\n",
    "        feature = data[:, 0:-1]\n",
    "        target = data[:, [-1]]\n",
    "    elif type(skip_index)== type(0):\n",
    "        feature = np.vstack((data[:skip_index, 0:-1], data[skip_index+1:, 0:-1]))\n",
    "        target = np.vstack((data[:skip_index, [-1]], data[skip_index+1:, [-1]]))\n",
    "    else:\n",
    "        print('Error!!!')\n",
    "    feature_target = np.hstack((feature, target))\n",
    "    print(feature_target.shape)\n",
    "    np.savetxt('datasets/filtered_embedding_vs_energy_env_' + str(env_no) + '.csv', feature_target, delimiter = \",\", fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46, 6)\n",
      "(46, 24)\n",
      "(46, 2000)\n",
      "(46, 24)\n"
     ]
    }
   ],
   "source": [
    "## reading data for Siamese\n",
    "\n",
    "dir_datasets = 'datasets/'\n",
    "#\n",
    "df_1_energy = pd.read_excel(dir_datasets + 'file_1.xlsx', sheet_name=0) ## energy sheet\n",
    "df_1_feature = genfromtxt(dir_datasets + 'embedding_vs_energy_env_0.csv', delimiter=',', dtype='float32')\n",
    "df_3_energy = pd.read_excel(dir_datasets + 'file_2.xlsx', sheet_name=0, engine='openpyxl') # pt111\n",
    "df_3_feature = genfromtxt(dir_datasets + 'embedding_vs_energy_env_0.csv', delimiter=',', dtype='float32')\n",
    "#\n",
    "m_1_energy = df_1_energy.values[1:]\n",
    "m_1_feature = df_1_feature[:,list(range(0,768))]\n",
    "m_3_energy = df_3_energy.values.T\n",
    "m_3_feature = df_3_feature[:,list(range(0,768))]\n",
    "#\n",
    "print(m_1_energy.shape)\n",
    "print(m_1_feature.shape)\n",
    "print(m_3_energy.shape)\n",
    "print(m_3_feature.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '127.0.0.1',\n",
       " 'raylet_ip_address': '127.0.0.1',\n",
       " 'redis_address': '127.0.0.1:6379',\n",
       " 'object_store_address': 'tcp://127.0.0.1:59817',\n",
       " 'raylet_socket_name': 'tcp://127.0.0.1:62400',\n",
       " 'webui_url': None,\n",
       " 'session_dir': 'C:\\\\Users\\\\mchowdh5\\\\AppData\\\\Local\\\\Temp\\\\ray\\\\session_2023-07-08_13-53-54_899690_12028',\n",
       " 'metrics_export_port': 62141,\n",
       " 'gcs_address': '127.0.0.1:55404',\n",
       " 'node_id': '63deb85a7e809b52420f3cb08144041a2932007f2ae24c6c6131c2d1'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.shutdown()\n",
    "ray.init(ignore_reinit_error=True, num_cpus=16) ## detects automatically: num_cpus=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## functions\n",
    "\n",
    "def get_best_models(\n",
    "    num_sample_envs=None, m_1_feature=None, m_1_energy=None, m_3_energy=None, skip_index_beef=None,\n",
    "    len_embedding=None, abstract_len_embedding=None, num_iterations=None, random_state=None\n",
    "):\n",
    "    list_index = list(range(2000))\n",
    "    if random_state is None:\n",
    "        random.shuffle(list_index)\n",
    "    else:\n",
    "        random.Random(random_state).shuffle(list_index)\n",
    "    list_sample_index = list_index[0: num_sample_envs]\n",
    "    \n",
    "    ## train: target-feature\n",
    "    data_feature_train = m_1_feature\n",
    "    data_target_train_core = np.array(m_1_energy[:,[3]], dtype='float32')\n",
    "    data_target_train = np.zeros((data_target_train_core.shape[0], num_sample_envs))\n",
    "    for i in range(data_target_train_core.shape[0]):\n",
    "        for j in range(num_sample_envs):\n",
    "            sample_index = list_sample_index[j]\n",
    "            mean = data_target_train_core[i][0]\n",
    "            deviation = m_3_energy[i][sample_index]\n",
    "            value = mean + deviation\n",
    "            data_target_train[i][j] = value\n",
    "    if skip_index_beef is not None:\n",
    "        data_feature_train = np.vstack((data_feature_train[:skip_index_beef, :], data_feature_train[skip_index_beef+1:, :]))\n",
    "        data_target_train = np.vstack((data_target_train[:skip_index_beef, :], data_target_train[skip_index_beef+1:, :]))  \n",
    "    #     print(data_feature_train.shape, data_target_train.shape)    \n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    ## environments: primary\n",
    "    environments = []\n",
    "    for i in range(data_target_train.shape[1]):\n",
    "        env = torch.from_numpy(data_feature_train[0:]), torch.from_numpy(data_target_train[0:, [i]])\n",
    "        environments.append(env)\n",
    "#     print(len(environments), environments[0][0].shape, environments[0][1].shape)\n",
    "    \n",
    "    # print('==============================================================================================================================')\n",
    "    # print(len(environments))\n",
    "    # for i, env in enumerate(environments):\n",
    "    #     features, targets = env\n",
    "    #     print(features.shape, targets.shape)\n",
    "    #     if torch.isnan(features).any():\n",
    "    #         print(f\"Environment {i} contains NaN values.\")\n",
    "    # print('==============================================================================================================================')\n",
    "    \n",
    "    \n",
    "    ## environments: secondary\n",
    "    senvironments = []\n",
    "    for env in environments:\n",
    "        senv = get_secondary_env(env=env)\n",
    "        senvironments.append(senv)\n",
    "#     print(len(senvironments), senvironments[0][0].shape, senvironments[0][1].shape)  \n",
    "    \n",
    "    ## model siamese\n",
    "    list_model_info = []\n",
    "    best_model_info, best_loss_siamese = (None, None, None, None), math.inf\n",
    "    #-----------change_1    \n",
    "    for _n_hidden_node in [128]: ## hint: select something that is divisible by 4  \n",
    "    #-----------change_1\n",
    "        for _lr in [1e-4, 1e-3, 1e-2, 1e-1, 1]: \n",
    "            for _struct_code in [1]:\n",
    "                model_siamese = SiameseNetwork(\n",
    "                    len_embedding, abstract_len_embedding, use_irm=False, n_hidden_node=_n_hidden_node,\n",
    "                    struct_code=_struct_code\n",
    "                )\n",
    "                optimizer_siamese = torch.optim.Adam(model_siamese.parameters(), lr=_lr)\n",
    "                \n",
    "                loss_siamese = None\n",
    "                for epoch in range(num_iterations):\n",
    "                    error_siamese = 0\n",
    "                    for x, y in senvironments:\n",
    "                        p = torch.randperm(len(x))\n",
    "                        x_e = x[p]\n",
    "                        y_e = y[p]\n",
    "                        fp1 = x_e[:, list(range(0, len_embedding, 1))]\n",
    "                        fp2 = x_e[:, list(range(len_embedding, 2 * len_embedding, 1))]\n",
    "                        y_pred_siamese = model_siamese(fp1, fp2) \n",
    "                        error_e_siamese = model_siamese.loss(y_pred_siamese, y_e)\n",
    "                        error_siamese += error_e_siamese\n",
    "                        \n",
    "                    error_siamese = error_siamese/len(senvironments)\n",
    "                    loss_siamese = 1 * error_siamese \n",
    "\n",
    "                    optimizer_siamese.zero_grad() ## clear buffer   \n",
    "                    loss_siamese.backward() ## calculate gradient for all params\n",
    "                    optimizer_siamese.step() ## update parameters using calculated gradients\n",
    "                \n",
    "                if loss_siamese < best_loss_siamese:\n",
    "                    best_model_info = (_n_hidden_node, _lr, _struct_code, model_siamese)\n",
    "                    best_loss_siamese = loss_siamese\n",
    "                file1 = open('logger.log', 'a+')  \n",
    "                file1.writelines(\n",
    "                    'n_hidden_node, lr, struct_code, loss_siamese, best_loss_siamese: ' \\\n",
    "                    + str(_n_hidden_node) + str(_lr) + str(_struct_code) \\\n",
    "                    + str(loss_siamese) + str(best_loss_siamese) + '\\n\\n'\n",
    "                )\n",
    "                file1.close()                \n",
    "                model_info = (_n_hidden_node, _lr, _struct_code, model_siamese)\n",
    "                list_model_info.append(model_info)\n",
    "             \n",
    "    return best_model_info, list_model_info        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote(num_returns=1)\n",
    "def get_result(\n",
    "    st, trial_no, list_env, dir_datasets, should_standardize, \n",
    "    num_sample_envs, m_1_feature, m_1_energy, m_3_energy, skip_index_beef, \n",
    "    len_embedding, num_iterations, pca_n_percent\n",
    "):\n",
    "    for env_no in list_env:\n",
    "        d_result = {}        \n",
    "        data = genfromtxt(dir_datasets + 'filtered_embedding_vs_energy_env_' + str(env_no) + '.csv', delimiter=',', dtype='float32')\n",
    "        feature = data[:, 0:-1]\n",
    "        target = data[:, [-1]]\n",
    "        if should_standardize:\n",
    "            scaler = StandardScaler().fit(feature)\n",
    "            feature = scaler.transform(feature)\n",
    "            \n",
    "        #####################################################################################################################################            \n",
    "        ## model 1: regular\n",
    "        model_name = 'regular' ## use the embeddings as it is\n",
    "        feature_regular, target_regular = feature, target\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            feature_regular, target_regular, test_size=0.33, random_state=trial_no\n",
    "        )        \n",
    "        ##\n",
    "        trainX, testX, trainY, testY = X_train, X_test, y_train.ravel(), y_test.ravel()\n",
    "        mlrun = utils.MLPredsByCV(cross_validation_split_no = 5)   \n",
    "        for alg in ['ridge','lasso','elastic','krr','svr']: ## ['ridge','lasso','elastic','krr','svr','gp']:\n",
    "            if alg == 'svr':\n",
    "                errors = mlrun.SVR_CV(trainX, testX, trainY, testY)\n",
    "            elif alg == 'krr':\n",
    "                errors = mlrun.KRR_CV(trainX, testX, trainY, testY)\n",
    "            elif alg == 'ridge':\n",
    "                errors = mlrun.Ridge_CV(trainX, testX, trainY, testY)\n",
    "            elif alg == 'lasso':\n",
    "                errors = mlrun.Lasso_CV(trainX, testX, trainY, testY)\n",
    "            elif alg == 'elastic':\n",
    "                errors = mlrun.Elastic_CV(trainX, testX, trainY, testY)\n",
    "            error = np.mean(errors)\n",
    "            key = (str(trial_no), str(env_no), str(model_name), str(alg))\n",
    "            d_result[key] = error\n",
    "        file1 = open('logger.log', 'a+')  \n",
    "        file1.writelines(\n",
    "            'time, trial_no, env_no, model_name: ' \\\n",
    "            + str(time.time()-st) + '=>  ' + str(trial_no) + ', ' \\\n",
    "            + str(env_no) + ', ' + str(model_name) + '\\n\\n'\n",
    "        )\n",
    "        file1.close()            \n",
    "        ##################################################################################################################################### \n",
    "        ## model 2: PCA\n",
    "        model_name = 'PCA' ## use the transformed embeddings by PCA\n",
    "        pca_dump_name = dir_datasets + 'pca_' + str(env_no) + '.pkl'\n",
    "        if should_standardize:\n",
    "            pca_dump_name = dir_datasets + 'pca_std_' + str(env_no) + '.pkl'                    \n",
    "        #-----------change_1\n",
    "        pca = PCA(n_components=pca_n_percent) ## e.g. 24 => 6 or other number of components\n",
    "        feature_train, feature_test, target_train, target_test = train_test_split(\n",
    "            feature, target, test_size=0.33, random_state=trial_no\n",
    "        )    \n",
    "        pca.fit(feature_train)\n",
    "        ## pickle dump\n",
    "        pk.dump(pca, open(pca_dump_name,\"wb\"))\n",
    "        ## later reload the pickle file\n",
    "        time.sleep(trial_no*4)           \n",
    "        pca = pk.load(open(pca_dump_name,\"rb\"))\n",
    "        #-----------change_1\n",
    "        pca_n_components = pca.n_components_\n",
    "        file1 = open('logger.log', 'a+')  \n",
    "        file1.writelines(str(pca_n_components) + '\\n\\n')\n",
    "        file1.close()            \n",
    "\n",
    "        feature_pca, target_pca = pca.transform(feature), target\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            feature_pca, target_pca, test_size=0.33, random_state=trial_no\n",
    "        )    \n",
    "        ##\n",
    "        trainX, testX, trainY, testY = X_train, X_test, y_train.ravel(), y_test.ravel()\n",
    "        mlrun = utils.MLPredsByCV(cross_validation_split_no = 5)                \n",
    "        for alg in ['ridge','lasso','elastic','krr','svr']: ## ['ridge','lasso','elastic','krr','svr','gp']:\n",
    "            if alg == 'svr':\n",
    "                errors = mlrun.SVR_CV(trainX, testX, trainY, testY)\n",
    "            elif alg == 'krr':\n",
    "                errors = mlrun.KRR_CV(trainX, testX, trainY, testY)\n",
    "            elif alg == 'ridge':\n",
    "                errors = mlrun.Ridge_CV(trainX, testX, trainY, testY)\n",
    "            elif alg == 'lasso':\n",
    "                errors = mlrun.Lasso_CV(trainX, testX, trainY, testY)\n",
    "            elif alg == 'elastic':\n",
    "                errors = mlrun.Elastic_CV(trainX, testX, trainY, testY)\n",
    "            error = np.mean(errors)\n",
    "            key = (str(trial_no), str(env_no), str(model_name), str(alg))\n",
    "            d_result[key] = error\n",
    "        file1 = open('logger.log', 'a+')  \n",
    "        file1.writelines(\n",
    "            'time, trial_no, env_no, model_name: ' \\\n",
    "            + str(time.time()-st) + '=>  ' + str(trial_no) + ', ' \\\n",
    "            + str(env_no) + ', ' + str(model_name) + '\\n\\n'\n",
    "        )\n",
    "        file1.close()            \n",
    "            \n",
    "        #####################################################################################################################################    \n",
    "        ## model 3: Siamese\n",
    "        pca_dump_name = dir_datasets + 'pca_' + str(env_no) + '.pkl'\n",
    "        if should_standardize:\n",
    "            pca_dump_name = dir_datasets + 'pca_std_' + str(env_no) + '.pkl'   \n",
    "        time.sleep(trial_no*6)                       \n",
    "        pca = pk.load(open(pca_dump_name,\"rb\"))\n",
    "        pca_n_components = pca.n_components_\n",
    "        #-----------------------------------------------------------------------------------\n",
    "#         filename_best_model_info = 'best_model_info_' + str(trial_no)\n",
    "#         filename_list_model_info = 'list_model_info_' + str(trial_no)        \n",
    "#         try:\n",
    "#             with open(\"datasets/\" + filename_best_model_info + \".pickle\", 'rb') as handle:\n",
    "#                 best_model_info = pk.load(handle)\n",
    "#             with open(\"datasets/\" + filename_list_model_info + \".pickle\", 'rb') as handle:\n",
    "#                 list_model_info = pk.load(handle)\n",
    "                \n",
    "#             file1 = open('logger.log', 'a+')  \n",
    "#             file1.writelines(\n",
    "#                 'trial_no: ' + str(trial_no) + ', Yes, saved model found!' + '\\n\\n'\n",
    "#             )\n",
    "#             file1.close()                            \n",
    "#         except Exception as e: \n",
    "#             file1 = open('logger.log', 'a+')  \n",
    "#             file1.writelines(\n",
    "#                 'trial_no: ' + str(trial_no) + ', No saved model found: '+ str(e) + '\\n\\n'\n",
    "#             )\n",
    "#             file1.close()            \n",
    "            \n",
    "#             best_model_info, list_model_info = get_best_models(\n",
    "#                 num_sample_envs=num_sample_envs, m_1_feature=m_1_feature, m_1_energy=m_1_energy,\n",
    "#                 m_3_energy=m_3_energy, skip_index_beef=skip_index_beef, len_embedding=len_embedding, abstract_len_embedding=pca_n_components,\n",
    "#                 num_iterations=num_iterations\n",
    "#             )\n",
    "#             with open(\"datasets/\" + filename_best_model_info + \".pickle\", 'wb') as handle: \n",
    "#                 pk.dump(best_model_info, handle, protocol=pk.HIGHEST_PROTOCOL)\n",
    "#             with open(\"datasets/\" + filename_list_model_info + \".pickle\", 'wb') as handle: \n",
    "#                 pk.dump(list_model_info, handle, protocol=pk.HIGHEST_PROTOCOL)\n",
    "        best_model_info, list_model_info = get_best_models(\n",
    "            num_sample_envs=num_sample_envs, m_1_feature=m_1_feature, m_1_energy=m_1_energy,\n",
    "            m_3_energy=m_3_energy, skip_index_beef=skip_index_beef, len_embedding=len_embedding, abstract_len_embedding=pca_n_components,\n",
    "            num_iterations=num_iterations, random_state=trial_no\n",
    "        )                            \n",
    "    \n",
    "        for model_info in [best_model_info]:            \n",
    "            _nhn, _lr, _sc, model_siamese = model_info[0], model_info[1], model_info[2], model_info[3]  \n",
    "            \n",
    "            # model_name = 'Siamese_' + str(_nhn) + '_' + str(_lr) + '_' + str(_sc) ## use the transformed embeddings by Siamese\n",
    "            model_name = 'Siamese'            \n",
    "            feature_siamese, target_siamese = model_siamese.forward_reg(torch.from_numpy(feature)).detach().numpy(), target \n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                feature_siamese, target_siamese, test_size=0.33, random_state=trial_no\n",
    "            )    \n",
    "        #-----------------------------------------------------------------------------------        \n",
    "        #\n",
    "            trainX, testX, trainY, testY = X_train, X_test, y_train.ravel(), y_test.ravel()\n",
    "            mlrun = utils.MLPredsByCV(cross_validation_split_no = 5)                \n",
    "            for alg in ['ridge','lasso','elastic','krr','svr']: ## ['ridge','lasso','elastic','krr','svr','gp']:\n",
    "                if alg == 'svr':\n",
    "                    errors = mlrun.SVR_CV(trainX, testX, trainY, testY)\n",
    "                elif alg == 'krr':\n",
    "                    errors = mlrun.KRR_CV(trainX, testX, trainY, testY)\n",
    "                elif alg == 'ridge':\n",
    "                    errors = mlrun.Ridge_CV(trainX, testX, trainY, testY)\n",
    "                elif alg == 'lasso':\n",
    "                    errors = mlrun.Lasso_CV(trainX, testX, trainY, testY)\n",
    "                elif alg == 'elastic':\n",
    "                    errors = mlrun.Elastic_CV(trainX, testX, trainY, testY)\n",
    "                error = np.mean(errors)\n",
    "                key = (str(trial_no), str(env_no), str(model_name), str(alg))\n",
    "                d_result[key] = error\n",
    "            file1 = open('logger.log', 'a+')  \n",
    "            file1.writelines('siamese: time, trial_no, env_no, model_name: ' + str(time.time()-st) + '=>  ' + str(trial_no) + ', ' + str(env_no) + ', ' + str(model_name) + '\\n\\n')\n",
    "            file1.close()            \n",
    "\n",
    "        #####################################################################################################################################\n",
    "        ## model 4: SiameseIRM\n",
    "        #####################################################################################################################################     \n",
    "        print('trial_no, env_no, time: ', trial_no, env_no, time.time()-st)    \n",
    "        \n",
    "        with open('datasets/d_result_' + str(trial_no) + '_' + str(env_no) + '.pickle', 'wb') as handle: \n",
    "            pk.dump(d_result, handle, protocol=pk.HIGHEST_PROTOCOL)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__=='__main__':\n",
    "    num_trials = 10\n",
    "    list_env = [0,1,2,3] \n",
    "    num_iterations = 2 \n",
    "    ##\n",
    "    num_sample_envs = 50\n",
    "    dir_datasets = 'datasets/'\n",
    "    should_standardize = False\n",
    "    skip_index_beef = None ## 41\n",
    "    len_embedding=768 ## change_2\n",
    "    pca_n_percent=0.90 ## change_2\n",
    "\n",
    "    ## main_1\n",
    "\n",
    "    st = time.time()\n",
    "    list_result_id = []\n",
    "    for trial_no in range(num_trials):\n",
    "        result_id = get_result.remote(\n",
    "            st, trial_no, list_env, dir_datasets, should_standardize, \n",
    "            num_sample_envs, m_1_feature, m_1_energy, m_3_energy, skip_index_beef, \n",
    "            len_embedding, num_iterations, pca_n_percent\n",
    "        )\n",
    "        list_result_id.append(result_id)\n",
    "    list_result = ray.get(list_result_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
